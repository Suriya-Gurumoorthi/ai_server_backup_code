{
  "threads": [
    {
      "id": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710",
      "createdAt": "2025-10-25T04:36:30.846Z",
      "modifiedAt": "2025-10-25T04:55:59.189Z",
      "remoteContextSelection": {
        "isRemoteContextSelectAll": true,
        "tags": []
      },
      "git": {
        "headBranch": "master",
        "targetBranch": "origin/master",
        "branches": [
          "origin/HEAD",
          "origin/dependabot/pip/scripts/requests-2.32.4",
          "origin/docs-dist-clean",
          "origin/fix_utf8_iteration",
          "origin/install-docs",
          "origin/master",
          "origin/multiword_toponyms",
          "origin/near_dupe_street_double_metaphones",
          "origin/parser-data",
          "origin/v1.1",
          "origin/whosonfirst-polygons"
        ],
        "diffMode": "diffWithLocal"
      },
      "mode": "code",
      "chatConversationMode": "agent"
    }
  ],
  "messages": [
    {
      "messageID": "757ae282-efba-4a05-892c-833b42628464",
      "threadID": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710",
      "createdAt": "2025-10-25T04:36:30.846Z",
      "mode": "freeChat",
      "role": "user",
      "command": {
        "command": "ask",
        "label": "/ask",
        "messageType": "text",
        "shortDescription": "Free-text questions",
        "description": "Free-text questions about your code",
        "placeholder": "What is your question",
        "showInWelcome": false
      },
      "userInput": "",
      "contextFiles": []
    },
    {
      "messageID": "a8d0b32f-7ae1-415d-8e12-69297c9f6a57",
      "originMessageID": "757ae282-efba-4a05-892c-833b42628464",
      "threadID": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710",
      "createdAt": "2025-10-25T04:36:30.846Z",
      "role": "assistant",
      "modelLabel": "GPT-5",
      "contextFiles": [],
      "command": {},
      "data": []
    },
    {
      "messageID": "132b7e12-ab35-42e5-a35c-269e8be75976",
      "threadID": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710",
      "createdAt": "2025-10-25T04:55:59.188Z",
      "mode": "code",
      "role": "user",
      "command": {
        "command": "ask",
        "label": "/ask",
        "messageType": "text",
        "shortDescription": "Free-text questions",
        "description": "Free-text questions about your code",
        "placeholder": "What is your question",
        "showInWelcome": false
      },
      "userInput": "go through the /server directory and /vicidial_bridge (22-10-2025), go throught he codebase thoroughly, and analyse any potential issue in the live calling system.\n\nlike if i interrupt, or speak in between, the model is getting response sometimes it is creating some random text in kannada or tamil when i interrupt, how can i fix it?",
      "contextFiles": []
    },
    {
      "messageID": "fcfd8ba7-bfc6-4035-9d2c-ff06d5536c7f",
      "originMessageID": "132b7e12-ab35-42e5-a35c-269e8be75976",
      "threadID": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710",
      "createdAt": "2025-10-25T04:55:59.188Z",
      "role": "assistant",
      "modelLabel": "GPT-5",
      "contextFiles": [],
      "command": {},
      "data": [
        {
          "id": "msg-587809",
          "subType": "code_analysis",
          "type": "text",
          "content": "<p>I will search the project to locate the referenced directories and scan relevant code paths that handle live calls, streaming ASR/LLM, and interruption handling to identify where random language output could be introduced. Then I’ll open and read key files to analyze logic and propose fixes.</p>",
          "originalContent": "I will search the project to locate the referenced directories and scan relevant code paths that handle live calls, streaming ASR/LLM, and interruption handling to identify where random language output could be introduced. Then I’ll open and read key files to analyze logic and propose fixes.",
          "isTyping": true
        },
        {
          "id": "call_IXWkoHR9FJmyyVgmdFm9YtzA",
          "type": "tool_use",
          "subType": "tool_use",
          "isTyping": false,
          "toolUseData": {
            "reasoning": " ",
            "tool": "list_files",
            "serverName": "File System",
            "args": "{\"path\":\"/home/novel\"}",
            "identifier": "call_IXWkoHR9FJmyyVgmdFm9YtzA",
            "pendingApproval": false,
            "serverType": "BUILTIN",
            "isError": false,
            "message": "{\"content\":[{\"type\":\"text\",\"text\":\"__pycache__/\\n.bash_history\\n.bash_logout\\n.bashrc\\n.cache/\\n.config/\\n.cursor/\\n.cursor-server/\\n.docker/\\n.dotnet/\\n.env\\n.git/\\n.git-credentials\\n.gitconfig\\n.gitignore\\n.ipython/\\n.lesshst\\n.lhotse/\\n.local/\\n.nv/\\n.ollama/\\n.profile\\n.python_history\\n.qodo/\\n.ssh/\\n.sudo_as_admin_successful\\n.triton/\\n.vscode-server/\\n.wget-hsts\\nAI_Interview/\\napikey_config/\\nASYNC_AWAIT_FIXES.md\\naudio_analysis_output/\\naudio_analysis_tts_response.py\\nAUDIO_PLACEHOLDER_FIXES.md\\ncall_recorder.py\\ncall_recordings/\\nchatterbox_tts.py\\ncheck_ultravox_models.py\\nconfig.py\\ndata/\\nen_US-lessac-medium\\nen_US-lessac-medium.json\\nen_US-lessac-medium.onnx\\nenhanced_tts_output/\\nenhanced_voice_clone_output/\\nespeak-ng-data/\\nhybrid_search/\\nlibpostal/\\nllm_example.py\\nLLM_SETUP_GUIDE.md\\nload_ultravox_model.py\\nmodel_client.py\\nmodel_loader.py\\nmodel_manager.py\\nmodel_server.py\\nmodel_storage_info.py\\nmodels/\\nMODULAR_PIPELINE_IMPLEMENTATION.md\\nmonitoring/\\npiper_tts.py\\npretrained_models/\\nPython-3.11.0.tgz\\npython312_voice_clone_tts.py\\nqwertyuiop/\\nREADME_audio_downloader.md\\nREADME.md\\nRecording.m4a\\nrequirements_vector.txt\\nrequirements.txt\\nrun_all_tests.py\\nsample-12s.wav\\nsample-3s.wav\\nsample-6s.wav\\nserver/\\nsetup_env.sh\\nsetup.sh\\nsimple_ollama_api/\\nsimple_tts.py\\nsnap/\\nsrc/\\nstart_server.sh\\ntemp_audio/\\ntest_folder/\\ntest-1.wav\\ntest-2.wav\\ntts_demo_output/\\nultravox_env/\\nultravox_env_detailed_packages.txt\\nultravox_env_documentation.md\\nultravox_env_packages_snapshot.txt\\nultravox_git/\\nvenv/\\nvici_digital_ultravox_api/\\nvicidial_bridge (08-10-2025)/\\nvicidial_bridge (22-10-2025)/\\nvoice_clone_output/\\nvoice_to_voice/\\nweavite_ragflow-main/\\nwhisper.py\"}],\"isError\":false}",
            "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
          },
          "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
        },
        {
          "id": "msg-271248",
          "subType": "code_analysis",
          "type": "text",
          "content": "<p>Next, I will list files inside the 'server' and 'vicidial_bridge (22-10-2025)' directories to map out the codebase and identify likely entry points for live call handling and interruption logic.</p>",
          "originalContent": "Next, I will list files inside the 'server' and 'vicidial_bridge (22-10-2025)' directories to map out the codebase and identify likely entry points for live call handling and interruption logic.",
          "isTyping": true
        },
        {
          "id": "call_MA6t2khWafd2NAtuIs86unLS",
          "type": "tool_use",
          "subType": "tool_use",
          "isTyping": false,
          "toolUseData": {
            "reasoning": " ",
            "tool": "list_files",
            "serverName": "File System",
            "args": "{\"path\":\"/home/novel/server\"}",
            "identifier": "call_MA6t2khWafd2NAtuIs86unLS",
            "pendingApproval": false,
            "serverType": "BUILTIN",
            "isError": false,
            "message": "{\"content\":[{\"type\":\"text\",\"text\":\"__pycache__/\\naudio_processor.py\\ncaptured_audio/\\nconfig.py\\nconversation_history/\\nimplementingRAGonPreviousVersion/\\nkpi_data/\\nmodels.py\\nprompt_logger.py\\nprompt_logs/\\nREADME.md\\nrequirements.txt\\nserver_code.py\\nserver.py\\ntest_audio_fix.py\\ntest_kpi_simple.py\\ntest_kpi_system.py\\ntest_silero_vad.py\\ntest_ultravad_direct.py\\ntest_vad_integration.py\\ntranscription_manager.py\\nutils.py\\nvad_manager.py\\nwebsocket_handler.py\"}],\"isError\":false}",
            "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
          },
          "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
        },
        {
          "id": "msg-798901",
          "subType": "code_analysis",
          "type": "text",
          "content": "<p>I will list the contents of the 'vicidial_bridge (22-10-2025)' directory to find the integration code that could influence the live call behavior.</p>",
          "originalContent": "I will list the contents of the 'vicidial_bridge (22-10-2025)' directory to find the integration code that could influence the live call behavior.",
          "isTyping": true
        },
        {
          "id": "call_qFeOeeaAkTSFaULodpLe11Mh",
          "type": "tool_use",
          "subType": "tool_use",
          "isTyping": false,
          "toolUseData": {
            "reasoning": " ",
            "tool": "list_files",
            "serverName": "File System",
            "args": "{\"path\":\"/home/novel/vicidial_bridge (22-10-2025)\"}",
            "identifier": "call_qFeOeeaAkTSFaULodpLe11Mh",
            "pendingApproval": false,
            "serverType": "BUILTIN",
            "isError": false,
            "message": "{\"content\":[{\"type\":\"text\",\"text\":\"__init__.py\\n__pycache__/\\nARCHITECTURE_DIAGRAM.md\\naudio_processing.py\\naudio_quality_analyzer.py\\nAUDIO_SOCKET_MIGRATION.md\\nbridge_methods.py\\nbridge.py\\ncall_recording.py\\nconfig.py\\nmain.py\\nREADME.md\\nrun_bridge.py\\ntext_to_speech.py\\nVAD_IMPROVEMENTS.md\\nvoice_activity_detection.py\\nwebsocket_client.py\"}],\"isError\":false}",
            "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
          },
          "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
        },
        {
          "id": "msg-397492",
          "subType": "code_analysis",
          "type": "text",
          "content": "<p>I will open and read key server and vicidial bridge files related to audio handling, ASR/LLM, and interruption logic to identify causes of random language output during barge-in.</p>",
          "originalContent": "I will open and read key server and vicidial bridge files related to audio handling, ASR/LLM, and interruption logic to identify causes of random language output during barge-in.",
          "isTyping": true
        },
        {
          "id": "call_ZZtcJd9G5aQaIl06ykjZGJtX",
          "type": "tool_use",
          "subType": "tool_use",
          "isTyping": false,
          "toolUseData": {
            "reasoning": " ",
            "tool": "read_file",
            "serverName": "File System",
            "args": "{\"path\":\"/home/novel/server/websocket_handler.py\"}",
            "identifier": "call_ZZtcJd9G5aQaIl06ykjZGJtX",
            "pendingApproval": false,
            "serverType": "BUILTIN",
            "isError": false,
            "message": "{\"content\":[{\"type\":\"text\",\"text\":\"\\\"\\\"\\\"\\nwebsocket_handler.py\\nWebSocket connection handler module.\\nManages WebSocket connections, message processing, and client state.\\n\\\"\\\"\\\"\\n\\nimport json\\nimport uuid\\nimport logging\\nimport asyncio\\nimport websockets\\nfrom typing import Dict, Any\\n\\nfrom config import (\\n    DEFAULT_CONVERSATION_TURNS, AVAILABLE_VOICES,\\n    TRANSCRIPTION_MIN_ENERGY, TRANSCRIPTION_MIN_SPEECH_RATIO,\\n    TRANSCRIPTION_FALSE_POSITIVES, TRANSCRIPTION_FALSE_POSITIVE_MAX_LENGTH,\\n    UNIFIED_AUDIO_VALIDATION, UNIFIED_MIN_ENERGY, UNIFIED_MIN_SPEECH_RATIO,\\n    STATIC_GREETING_ENABLED, STATIC_GREETING_MESSAGE,\\n    VAD_ENABLED, VAD_BACKGROUND_MONITORING_ENABLED, VAD_INTERRUPTION_COOLDOWN_MS,\\n    VAD_TEMPORAL_CONSISTENCY_FRAMES, VAD_DEBUG_LOGGING\\n)\\nfrom utils import safe_send_response, get_connection_info, should_transcribe_audio, unified_audio_validation\\nfrom models import model_manager\\nfrom audio_processor import audio_processor\\nfrom transcription_manager import transcription_manager\\nfrom prompt_logger import prompt_logger\\nfrom vad_manager import vad_manager\\n\\n\\nclass WebSocketHandler:\\n    \\\"\\\"\\\"Handles WebSocket connections and message processing.\\\"\\\"\\\"\\n    \\n    def __init__(self):\\n        self.logger = logging.getLogger(__name__)\\n        self.active_connections: Dict[str, Dict[str, Any]] = {}\\n    \\n    def _should_process_audio(self, audio_bytes: bytes) -> bool:\\n        \\\"\\\"\\\"\\n        Unified audio validation for all processing paths.\\n        Ensures Whisper and Ultravox are synchronized using the same strict criteria.\\n        \\\"\\\"\\\"\\n        if not UNIFIED_AUDIO_VALIDATION:\\n            return True  # Skip validation if disabled\\n        \\n        # Use unified validation with the same strict criteria for both Whisper and Ultravox\\n        result = unified_audio_validation(\\n            audio_bytes, \\n            min_energy=UNIFIED_MIN_ENERGY, \\n            min_speech_ratio=UNIFIED_MIN_SPEECH_RATIO\\n        )\\n        \\n        # Debug logging\\n        self.logger.info(f\\\"[UNIFIED] Audio validation result: {result} (energy={UNIFIED_MIN_ENERGY}, speech_ratio={UNIFIED_MIN_SPEECH_RATIO})\\\")\\n        \\n        return result\\n    \\n    async def _send_static_greeting(self, connection_id: str, websocket, client_address: str):\\n        \\\"\\\"\\\"Send the static greeting message immediately after connection.\\\"\\\"\\\"\\n        try:\\n            if connection_id not in self.active_connections:\\n                self.logger.error(f\\\"Connection {connection_id} not found for greeting\\\")\\n                return\\n            \\n            connection = self.active_connections[connection_id]\\n            \\n            # Check if greeting has already been sent\\n            if connection.get(\\\"greeting_sent\\\", False):\\n                self.logger.info(f\\\"Greeting already sent for connection {connection_id}\\\")\\n                return\\n            \\n            # CRITICAL FIX: Create session BEFORE sending greeting to ensure it's stored in history\\n            if not connection.get(\\\"session_created\\\", False):\\n                session_id = transcription_manager.create_session(connection_id)\\n                connection[\\\"session_id\\\"] = session_id\\n                connection[\\\"session_created\\\"] = True\\n                \\n                # Create prompt logging session\\n                prompt_logger.create_session(connection_id, session_id)\\n                \\n                self.logger.info(f\\\"Created session {session_id} for connection {connection_id} for greeting storage\\\")\\n            \\n            self.logger.info(f\\\"🎤 Sending static greeting to {client_address}: {STATIC_GREETING_MESSAGE}\\\")\\n            \\n            # Generate TTS audio for the greeting message\\n            greeting_audio = await audio_processor.generate_tts_audio(STATIC_GREETING_MESSAGE)\\n            \\n            if greeting_audio:\\n                # Send the greeting audio directly to the client\\n                await websocket.send(greeting_audio)\\n                self.logger.info(f\\\"✅ Static greeting audio sent to {client_address} ({len(greeting_audio)} bytes)\\\")\\n            else:\\n                # Fallback: send text response if TTS fails\\n                self.logger.warning(f\\\"TTS failed for greeting, sending text to {client_address}\\\")\\n                await safe_send_response(websocket, STATIC_GREETING_MESSAGE, client_address)\\n            \\n            # Mark greeting as sent\\n            connection[\\\"greeting_sent\\\"] = True\\n            \\n            # CRITICAL FIX: Always add greeting to conversation history since session is now created\\n            transcription_manager.add_ai_transcription(connection_id, STATIC_GREETING_MESSAGE)\\n            self.logger.info(f\\\"✅ Added greeting to conversation history for {connection_id}\\\")\\n            \\n        except Exception as e:\\n            self.logger.error(f\\\"Error sending static greeting to {client_address}: {e}\\\")\\n            # Don't fail the connection if greeting fails\\n    \\n    def _ensure_session_created(self, connection_id: str) -> bool:\\n        \\\"\\\"\\\"Ensure a session is created for the connection if it doesn't exist yet.\\\"\\\"\\\"\\n        connection = self.active_connections.get(connection_id)\\n        if not connection:\\n            return False\\n        \\n        # Check if session already exists (created during greeting or previous message)\\n        if connection.get(\\\"session_created\\\", False):\\n            self.logger.info(f\\\"Session already exists for connection {connection_id}\\\")\\n            return True\\n        \\n        # Create session only if it doesn't exist yet\\n        session_id = transcription_manager.create_session(connection_id)\\n        connection[\\\"session_id\\\"] = session_id\\n        connection[\\\"session_created\\\"] = True\\n        \\n        # Create prompt logging session\\n        prompt_logger.create_session(connection_id, session_id)\\n        \\n        self.logger.info(f\\\"Created session {session_id} for connection {connection_id} on first message\\\")\\n        return True\\n    \\n    async def handle_connection(self, websocket):\\n        \\\"\\\"\\\"Handle a new WebSocket connection.\\\"\\\"\\\"\\n        connection_id = str(uuid.uuid4())\\n        client_address = get_connection_info(websocket)\\n        \\n        try:\\n            # Initialize connection state without creating session yet\\n            # Session will be created only when first conversation message is received\\n            self.active_connections[connection_id] = {\\n                \\\"websocket\\\": websocket,\\n                \\\"client_address\\\": client_address,\\n                \\\"conversation_turns\\\": DEFAULT_CONVERSATION_TURNS.copy(),\\n                \\\"waiting_for_audio\\\": False,\\n                \\\"pending_request_type\\\": None,\\n                \\\"session_id\\\": None,  # Will be set when first message is processed\\n                \\\"session_created\\\": False,  # Track if session has been created\\n                \\\"greeting_sent\\\": False,  # Track if static greeting has been sent\\n                # VAD interruption detection state\\n                \\\"ai_speaking\\\": False,  # Track if AI is currently speaking\\n                \\\"interruption_detected\\\": False,  # Track if interruption was detected\\n                \\\"last_interruption_time\\\": 0,  # Timestamp of last interruption (for cooldown)\\n                \\\"vad_monitoring_task\\\": None,  # Background VAD monitoring task\\n                \\\"audio_buffer\\\": []  # Buffer for incoming audio during AI speech\\n            }\\n            \\n            self.logger.info(f\\\"✅ Client {client_address} connected with ID: {connection_id}\\\")\\n            self.logger.info(f\\\"📊 Active connections: {len(self.active_connections)}\\\")\\n            \\n            # Send static greeting message immediately after connection\\n            if STATIC_GREETING_ENABLED:\\n                await self._send_static_greeting(connection_id, websocket, client_address)\\n            \\n            # Process messages from the client\\n            async for message in websocket:\\n                try:\\n                    await self.process_message(connection_id, message)\\n                except Exception as e:\\n                    self.logger.error(f\\\"❌ Error processing message from {client_address}: {e}\\\")\\n                    error_response = f\\\"Error processing input: {str(e)}\\\"\\n                    await safe_send_response(websocket, error_response, client_address)\\n\\n        except websockets.ConnectionClosedOK:\\n            self.logger.info(f\\\"👋 Client {client_address} disconnected gracefully.\\\")\\n        except websockets.ConnectionClosedError as e:\\n            self.logger.warning(f\\\"⚠️  Client {client_address} connection closed with error: {e}\\\")\\n        except Exception as e:\\n            self.logger.error(f\\\"💥 Unexpected connection error for {client_address}: {e}\\\")\\n        finally:\\n            # Clean up connection state and end transcription session\\n            if connection_id in self.active_connections:\\n                connection = self.active_connections[connection_id]\\n                \\n                # Cancel VAD monitoring task if active\\n                if connection.get(\\\"vad_monitoring_task\\\"):\\n                    connection[\\\"vad_monitoring_task\\\"].cancel()\\n                    try:\\n                        await connection[\\\"vad_monitoring_task\\\"]\\n                    except asyncio.CancelledError:\\n                        pass\\n                \\n                # Cancel any active TTS generation\\n                if audio_processor.is_tts_active(connection_id):\\n                    audio_processor.cancel_tts(connection_id)\\n                \\n                # Only end session if it was actually created\\n                if connection.get(\\\"session_created\\\", False):\\n                    await transcription_manager.end_session(connection_id)\\n                    prompt_logger.end_session(connection_id)\\n                del self.active_connections[connection_id]\\n            self.logger.info(f\\\"🧹 Cleaned up connection {connection_id}. Active connections: {len(self.active_connections)}\\\")\\n    \\n    async def process_message(self, connection_id: str, message):\\n        \\\"\\\"\\\"Process a single message for a specific connection.\\\"\\\"\\\"\\n        connection = self.active_connections.get(connection_id)\\n        if not connection:\\n            self.logger.error(f\\\"Connection {connection_id} not found in active connections\\\")\\n            return\\n        \\n        websocket = connection[\\\"websocket\\\"]\\n        client_address = connection[\\\"client_address\\\"]\\n        \\n        self.logger.info(f\\\"Received message of type {type(message)} from {client_address}\\\")\\n\\n        # Handle binary audio messages\\n        if isinstance(message, bytes):\\n            await self._handle_audio_message(connection, message)\\n            return\\n\\n        # Handle text/JSON messages\\n        if isinstance(message, str):\\n            await self._handle_text_message(connection, message)\\n            return\\n    \\n    async def _handle_audio_message(self, connection: Dict[str, Any], audio_bytes: bytes):\\n        \\\"\\\"\\\"Handle binary audio message with unified validation.\\\"\\\"\\\"\\n        websocket = connection[\\\"websocket\\\"]\\n        client_address = connection[\\\"client_address\\\"]\\n        \\n        # Find the connection_id for this connection\\n        connection_id = None\\n        for conn_id, conn_data in self.active_connections.items():\\n            if conn_data == connection:\\n                connection_id = conn_id\\n                break\\n        \\n        if not connection_id:\\n            self.logger.error(f\\\"Could not find connection ID for {client_address}\\\")\\n            await safe_send_response(websocket, \\\"Error: Connection not found\\\", client_address)\\n            return\\n        \\n        # CRITICAL FIX: Apply unified validation FIRST, before any AI processing\\n        if not self._should_process_audio(audio_bytes):\\n            self.logger.info(f\\\"Audio rejected at validation gate - no processing at all\\\")\\n            # Don't process, don't log, don't add to history\\n            # Optionally send a user-facing message (optional)\\n            return\\n        \\n        # Only after validation passes, proceed with the rest\\n        if not self._ensure_session_created(connection_id):\\n            self.logger.error(f\\\"Failed to create session for connection {connection_id}\\\")\\n            await safe_send_response(websocket, \\\"Error: Failed to create session\\\", client_address)\\n            return\\n        \\n        if connection[\\\"waiting_for_audio\\\"]:\\n            # Process the audio with the pending request type\\n            request_type = connection[\\\"pending_request_type\\\"]\\n            connection[\\\"waiting_for_audio\\\"] = False\\n            connection[\\\"pending_request_type\\\"] = None\\n            \\n            if request_type == \\\"transcribe\\\":\\n                await self._process_transcribe_request(websocket, client_address, audio_bytes, connection)\\n            elif request_type == \\\"features\\\":\\n                await self._process_features_request(websocket, client_address, audio_bytes)\\n            elif request_type == \\\"tts\\\":\\n                await self._process_tts_request(websocket, client_address, audio_bytes, connection)\\n            else:\\n                self.logger.warning(f\\\"Unknown request type '{request_type}' for {client_address}\\\")\\n                await safe_send_response(websocket, f\\\"Error: Unknown request type '{request_type}'\\\", client_address)\\n        else:\\n            # Direct audio processing\\n            self.logger.info(f\\\"[GATE] Audio passed gate validation - processing\\\")\\n            \\n            conversation_context = transcription_manager.get_conversation_context_for_ai(connection_id)\\n            combined_turns = []\\n            \\n            if connection[\\\"conversation_turns\\\"] and connection[\\\"conversation_turns\\\"][0].get(\\\"role\\\") == \\\"system\\\":\\n                combined_turns.append(connection[\\\"conversation_turns\\\"][0])\\n            \\n            combined_turns.extend(conversation_context)\\n            \\n            # Call Ultravox - it will apply TIER 2 validation internally\\n            response_text = await model_manager.process_audio(audio_bytes, combined_turns, connection_id)\\n            \\n            # Check if response is a rejection marker\\n            if response_text.startswith(\\\"[AUDIO_REJECTED_BY_ULTRAVOX_VALIDATION]\\\"):\\n                self.logger.info(f\\\"[ULTRAVOX] Audio rejected after gate - borderline audio\\\")\\n                # Don't send anything to user, don't log to history, just return\\n                return\\n            \\n            if response_text.startswith(\\\"[\\\"):\\n                # Other internal errors\\n                self.logger.error(f\\\"[ULTRAVOX] Internal error: {response_text}\\\")\\n                # Don't expose internal errors to user, don't log\\n                return\\n            \\n            self.logger.info(f\\\"[ULTRAVOX] AI Response: {response_text[:100]}...\\\")\\n            \\n            # Generate TTS with interruption support\\n            if VAD_ENABLED and VAD_BACKGROUND_MONITORING_ENABLED:\\n                # Use cancellable TTS generation\\n                tts_audio = await audio_processor.generate_tts_audio_cancellable(response_text, connection_id)\\n                \\n                if tts_audio:\\n                    # Mark AI as speaking and start VAD monitoring\\n                    connection[\\\"ai_speaking\\\"] = True\\n                    connection[\\\"interruption_detected\\\"] = False\\n                    \\n                    # Start VAD monitoring task\\n                    vad_task = asyncio.create_task(\\n                        self._monitor_for_interruption(connection_id, websocket)\\n                    )\\n                    connection[\\\"vad_monitoring_task\\\"] = vad_task\\n                    \\n                    # Send TTS audio\\n                    await websocket.send(tts_audio)\\n                    \\n                    # Wait for VAD monitoring to complete or be cancelled\\n                    try:\\n                        await vad_task\\n                    except asyncio.CancelledError:\\n                        self.logger.info(f\\\"VAD monitoring cancelled for {client_address}\\\")\\n                    finally:\\n                        connection[\\\"ai_speaking\\\"] = False\\n                        connection[\\\"vad_monitoring_task\\\"] = None\\n                        connection[\\\"audio_buffer\\\"] = []\\n                else:\\n                    await safe_send_response(websocket, response_text, client_address)\\n            else:\\n                # Use standard TTS generation (no interruption support)\\n                tts_audio = await audio_processor.generate_tts_audio(response_text)\\n                \\n                if tts_audio:\\n                    await websocket.send(tts_audio)\\n                else:\\n                    await safe_send_response(websocket, response_text, client_address)\\n            \\n            # Add AI response to history ONLY if it's valid and not a marker\\n            transcription_manager.add_ai_transcription(connection_id, response_text)\\n            \\n            # Background transcription for logging\\n            async def log_transcription(audio, conn_id):\\n                try:\\n                    user_transcription = await model_manager.transcribe_audio(audio, conn_id)\\n                    \\n                    if user_transcription and len(user_transcription.strip()) > 0:\\n                        transcription_lower = user_transcription.strip().lower()\\n                        if transcription_lower not in TRANSCRIPTION_FALSE_POSITIVES:\\n                            transcription_manager.add_user_transcription(conn_id, user_transcription)\\n                            self.logger.info(f\\\"[BACKGROUND] User transcription logged: {user_transcription[:50]}...\\\")\\n                        else:\\n                            self.logger.info(f\\\"[BACKGROUND] Rejected false positive: '{user_transcription}'\\\")\\n                except Exception as e:\\n                    self.logger.error(f\\\"[BACKGROUND] Transcription error: {e}\\\")\\n            \\n            asyncio.create_task(log_transcription(audio_bytes, connection_id))\\n    \\n    async def _handle_text_message(self, connection: Dict[str, Any], message: str):\\n        \\\"\\\"\\\"Handle text/JSON message.\\\"\\\"\\\"\\n        websocket = connection[\\\"websocket\\\"]\\n        client_address = connection[\\\"client_address\\\"]\\n        \\n        try:\\n            request = json.loads(message)\\n            req_type = request.get(\\\"type\\\", \\\"transcribe\\\")\\n        except Exception:\\n            await websocket.send(\\\"Error: Cannot parse non-audio, non-JSON input.\\\")\\n            return\\n\\n        # Handle different request types\\n        if req_type == \\\"transcribe\\\":\\n            connection[\\\"waiting_for_audio\\\"] = True\\n            connection[\\\"pending_request_type\\\"] = \\\"transcribe\\\"\\n            self.logger.info(f\\\"Set waiting for audio for {client_address}, request type: transcribe\\\")\\n\\n        elif req_type == \\\"features\\\":\\n            connection[\\\"waiting_for_audio\\\"] = True\\n            connection[\\\"pending_request_type\\\"] = \\\"features\\\"\\n            self.logger.info(f\\\"Set waiting for audio for {client_address}, request type: features\\\")\\n\\n        elif req_type == \\\"tts\\\":\\n            connection[\\\"waiting_for_audio\\\"] = True\\n            connection[\\\"pending_request_type\\\"] = \\\"tts\\\"\\n            self.logger.info(f\\\"Set waiting for audio for {client_address}, request type: tts\\\")\\n\\n        elif req_type == \\\"voices\\\":\\n            voices_info = {\\\"voices\\\": AVAILABLE_VOICES}\\n            self.logger.info(f\\\"Sending voices response to {client_address}\\\")\\n            await safe_send_response(websocket, json.dumps(voices_info), client_address)\\n\\n        else:\\n            self.logger.warning(f\\\"Unsupported request type '{req_type}' from {client_address}\\\")\\n            await safe_send_response(websocket, f\\\"Error: Unsupported request type '{req_type}'\\\", client_address)\\n    \\n    def _detect_and_update_candidate_name(self, connection_id: str, user_transcription: str):\\n        \\\"\\\"\\\"Detect if user provided their name and update session state.\\\"\\\"\\\"\\n        import re\\n        \\n        # Common patterns for name introduction\\n        name_patterns = [\\n            r\\\"my name is (\\\\w+)\\\",\\n            r\\\"i am (\\\\w+)\\\",\\n            r\\\"i'm (\\\\w+)\\\",\\n            r\\\"call me (\\\\w+)\\\",\\n            r\\\"this is (\\\\w+)\\\",\\n            r\\\"(\\\\w+) here\\\",\\n            r\\\"(\\\\w+) speaking\\\"\\n        ]\\n        \\n        user_text = user_transcription.lower().strip()\\n        \\n        for pattern in name_patterns:\\n            match = re.search(pattern, user_text, re.IGNORECASE)\\n            if match:\\n                name = match.group(1).strip()\\n                # Filter out common false positives\\n                if name not in [\\\"alex\\\", \\\"alexa\\\", \\\"rachel\\\", \\\"emily\\\", \\\"richard\\\", \\\"sir\\\", \\\"madam\\\", \\\"yes\\\", \\\"no\\\", \\\"okay\\\", \\\"ok\\\"]:\\n                    transcription_manager.update_candidate_name(connection_id, name)\\n                    self.logger.info(f\\\"Detected candidate name: {name} for connection {connection_id}\\\")\\n                    break\\n    \\n    async def _process_transcribe_request(self, websocket, client_address: str, audio_bytes: bytes, connection: Dict[str, Any]):\\n        \\\"\\\"\\\"Process transcribe request with parallel processing and audio validation.\\\"\\\"\\\"\\n        connection_id = None\\n        for conn_id, conn_data in self.active_connections.items():\\n            if conn_data == connection:\\n                connection_id = conn_id\\n                break\\n        \\n        if not connection_id:\\n            self.logger.error(f\\\"Could not find connection ID for {client_address}\\\")\\n            await safe_send_response(websocket, \\\"Error: Connection not found\\\", client_address)\\n            return\\n        \\n        self.logger.info(f\\\"Processing transcribe request for {client_address}\\\")\\n        \\n        # VALIDATION: Check if audio is worth transcribing (unified validation)\\n        if not self._should_process_audio(audio_bytes):\\n            self.logger.info(f\\\"Rejecting transcribe request - audio appears to be noise/silence\\\")\\n            await safe_send_response(websocket, \\\"[No speech detected]\\\", client_address)\\n            return\\n        \\n        # -- Critical Path: Transcribe user input first, then generate AI response --\\n        self.logger.info(f\\\"Processing audio for AI response for {client_address}\\\")\\n        \\n        # STEP 1: Transcribe user input first (this is critical for proper context)\\n        self.logger.info(f\\\"Transcribing user input for {client_address}\\\")\\n        user_transcription = await model_manager.transcribe_audio(audio_bytes, connection_id)\\n        \\n        # Validate transcription result\\n        if not user_transcription or len(user_transcription.strip()) == 0:\\n            self.logger.warning(f\\\"Empty transcription result, treating as silence\\\")\\n            await safe_send_response(websocket, \\\"[No speech detected]\\\", client_address)\\n            return\\n        \\n        # Check for common false positives\\n        transcription_lower = user_transcription.strip().lower()\\n        if transcription_lower in TRANSCRIPTION_FALSE_POSITIVES and len(audio_bytes) < 8000:\\n            # Likely false positive if transcription is short AND audio is brief\\n            self.logger.warning(f\\\"Likely false positive detected: '{user_transcription}' (matched: {transcription_lower}) - rejecting\\\")\\n            await safe_send_response(websocket, \\\"[Background noise filtered]\\\", client_address)\\n            return\\n        \\n        transcription_manager.add_user_transcription(connection_id, user_transcription)\\n        \\n        # Check if user provided their name and update session state\\n        self._detect_and_update_candidate_name(connection_id, user_transcription)\\n        \\n        self.logger.info(f\\\"User transcription: {user_transcription[:50]}...\\\")\\n        \\n        # STEP 2: Get updated conversation context (now includes current user input)\\n        conversation_context = transcription_manager.get_conversation_context_for_ai(connection_id)\\n        \\n        # Combine with default system prompt\\n        combined_turns = connection[\\\"conversation_turns\\\"].copy()\\n        combined_turns.extend(conversation_context)\\n        \\n        # STEP 3: Generate AI response with proper context\\n        response_text = await model_manager.process_audio(audio_bytes, combined_turns, connection_id)\\n        self.logger.info(f\\\"AI Response: {response_text[:100]}...\\\")\\n        \\n        # STEP 4: Send response to user\\n        await safe_send_response(websocket, response_text, client_address)\\n        \\n        # STEP 5: Add AI response to history\\n        transcription_manager.add_ai_transcription(connection_id, response_text)\\n    \\n    async def _process_features_request(self, websocket, client_address: str, audio_bytes: bytes):\\n        \\\"\\\"\\\"Process features request with audio validation.\\\"\\\"\\\"\\n        self.logger.info(f\\\"Processing features request for {client_address}\\\")\\n        \\n        # VALIDATION: Check if audio is worth processing (unified validation)\\n        if not self._should_process_audio(audio_bytes):\\n            self.logger.info(f\\\"Rejecting features request - audio appears to be noise/silence\\\")\\n            await safe_send_response(websocket, \\\"[No speech detected - unable to analyze features]\\\", client_address)\\n            return\\n        \\n        features_prompt = \\\"Transcribe the audio and also provide speaker gender, emotion, accent, and audio quality.\\\"\\n        turns = [{\\\"role\\\": \\\"system\\\", \\\"content\\\": features_prompt}]\\n        response_text = await model_manager.process_audio(audio_bytes, turns)\\n        self.logger.info(f\\\"Sending features response to {client_address}\\\")\\n        await safe_send_response(websocket, response_text, client_address)\\n    \\n    async def _process_tts_request(self, websocket, client_address: str, audio_bytes: bytes, connection: Dict[str, Any]):\\n        \\\"\\\"\\\"Process TTS request with parallel processing and audio validation.\\\"\\\"\\\"\\n        connection_id = None\\n        for conn_id, conn_data in self.active_connections.items():\\n            if conn_data == connection:\\n                connection_id = conn_id\\n                break\\n        \\n        if not connection_id:\\n            self.logger.error(f\\\"Could not find connection ID for {client_address}\\\")\\n            await safe_send_response(websocket, \\\"Error: Connection not found\\\", client_address)\\n            return\\n        \\n        self.logger.info(f\\\"Processing TTS request for {client_address}\\\")\\n        \\n        # VALIDATION: Check if audio is worth transcribing (unified validation)\\n        if not self._should_process_audio(audio_bytes):\\n            self.logger.info(f\\\"Rejecting TTS request - audio appears to be noise/silence\\\")\\n            # Generate a brief \\\"I didn't catch that\\\" response\\n            response_text = \\\"I didn't catch that. Could you please repeat?\\\"\\n            tts_audio = await audio_processor.generate_tts_audio(response_text)\\n            if tts_audio:\\n                await websocket.send(tts_audio)\\n            else:\\n                await safe_send_response(websocket, response_text, client_address)\\n            return\\n        \\n        # -- Critical Path: Transcribe user input first, then generate AI response --\\n        self.logger.info(f\\\"Processing audio for AI response for {client_address}\\\")\\n        \\n        # STEP 1: Transcribe user input first\\n        self.logger.info(f\\\"Transcribing user input for {client_address}\\\")\\n        user_transcription = await model_manager.transcribe_audio(audio_bytes, connection_id)\\n        \\n        # Validate transcription result\\n        if not user_transcription or len(user_transcription.strip()) == 0:\\n            self.logger.warning(f\\\"Empty transcription result\\\")\\n            response_text = \\\"I didn't catch that. Could you please repeat?\\\"\\n            tts_audio = await audio_processor.generate_tts_audio(response_text)\\n            if tts_audio:\\n                await websocket.send(tts_audio)\\n            else:\\n                await safe_send_response(websocket, response_text, client_address)\\n            return\\n        \\n        # Check for common false positives\\n        transcription_lower = user_transcription.strip().lower()\\n        if transcription_lower in TRANSCRIPTION_FALSE_POSITIVES and len(audio_bytes) < 8000:\\n            self.logger.warning(f\\\"Likely false positive: '{user_transcription}' (matched: {transcription_lower}) - requesting clarification\\\")\\n            response_text = \\\"I didn't quite catch that. Could you please speak a bit louder?\\\"\\n            tts_audio = await audio_processor.generate_tts_audio(response_text)\\n            if tts_audio:\\n                await websocket.send(tts_audio)\\n            else:\\n                await safe_send_response(websocket, response_text, client_address)\\n            return\\n        \\n        transcription_manager.add_user_transcription(connection_id, user_transcription)\\n        \\n        # Check if user provided their name\\n        self._detect_and_update_candidate_name(connection_id, user_transcription)\\n        \\n        self.logger.info(f\\\"User transcription: {user_transcription[:50]}...\\\")\\n        \\n        # STEP 2-5: Continue with normal processing...\\n        conversation_context = transcription_manager.get_conversation_context_for_ai(connection_id)\\n        combined_turns = connection[\\\"conversation_turns\\\"].copy()\\n        combined_turns.extend(conversation_context)\\n        \\n        response_text = await model_manager.process_audio(audio_bytes, combined_turns, connection_id)\\n        self.logger.info(f\\\"AI Response: {response_text[:100]}...\\\")\\n        \\n        transcription_manager.add_ai_transcription(connection_id, response_text)\\n        \\n        # Generate TTS audio\\n        self.logger.info(f\\\"Generating TTS audio for {client_address}\\\")\\n        tts_audio = await audio_processor.generate_tts_audio(response_text)\\n        \\n        if tts_audio:\\n            await websocket.send(tts_audio)\\n            self.logger.info(f\\\"Successfully sent TTS audio to {client_address}\\\")\\n        else:\\n            await safe_send_response(websocket, response_text, client_address)\\n    \\n    async def _monitor_for_interruption(self, connection_id: str, websocket):\\n        \\\"\\\"\\\"\\n        Monitor for user speech during AI TTS playback using VAD.\\n        \\n        Args:\\n            connection_id: Connection identifier\\n            websocket: WebSocket connection\\n        \\\"\\\"\\\"\\n        connection = self.active_connections.get(connection_id)\\n        if not connection:\\n            self.logger.error(f\\\"Connection {connection_id} not found for VAD monitoring\\\")\\n            return\\n        \\n        client_address = connection[\\\"client_address\\\"]\\n        self.logger.info(f\\\"Starting VAD monitoring for {client_address}\\\")\\n        \\n        try:\\n            audio_chunks = []\\n            consecutive_speech_frames = 0\\n            last_interruption_time = connection.get(\\\"last_interruption_time\\\", 0)\\n            \\n            while connection.get(\\\"ai_speaking\\\", False) and not connection.get(\\\"interruption_detected\\\", False):\\n                try:\\n                    # Wait for incoming audio with short timeout\\n                    audio_chunk = await asyncio.wait_for(websocket.recv(), timeout=0.1)\\n                    \\n                    if isinstance(audio_chunk, bytes):\\n                        # Buffer audio chunk for VAD analysis\\n                        audio_chunks.append(audio_chunk)\\n                        connection[\\\"audio_buffer\\\"].append(audio_chunk)\\n                        \\n                        # Keep only recent chunks to avoid memory buildup\\n                        if len(audio_chunks) > 10:  # Keep last 10 chunks (~300ms)\\n                            audio_chunks.pop(0)\\n                        \\n                        # Check for speech with temporal consistency\\n                        if len(audio_chunks) >= VAD_TEMPORAL_CONSISTENCY_FRAMES:\\n                            is_speech, avg_prob = vad_manager.detect_speech_with_temporal_consistency(\\n                                audio_chunks[-VAD_TEMPORAL_CONSISTENCY_FRAMES:],\\n                                VAD_TEMPORAL_CONSISTENCY_FRAMES\\n                            )\\n                            \\n                            if VAD_DEBUG_LOGGING:\\n                                self.logger.debug(f\\\"VAD analysis: is_speech={is_speech}, avg_prob={avg_prob:.3f}\\\")\\n                            \\n                            if is_speech:\\n                                # Check cooldown period\\n                                current_time = asyncio.get_event_loop().time() * 1000  # Convert to ms\\n                                time_since_last = current_time - last_interruption_time\\n                                \\n                                if time_since_last >= VAD_INTERRUPTION_COOLDOWN_MS:\\n                                    self.logger.info(f\\\"🚨 Interruption detected for {client_address} (prob={avg_prob:.3f})\\\")\\n                                    \\n                                    # Trigger interruption\\n                                    await self._trigger_interruption(connection_id, websocket)\\n                                    break\\n                                else:\\n                                    if VAD_DEBUG_LOGGING:\\n                                        self.logger.debug(f\\\"Interruption ignored due to cooldown: {time_since_last:.0f}ms < {VAD_INTERRUPTION_COOLDOWN_MS}ms\\\")\\n                            else:\\n                                consecutive_speech_frames = 0\\n                    \\n                except asyncio.TimeoutError:\\n                    # No audio received, continue monitoring\\n                    continue\\n                except websockets.ConnectionClosed:\\n                    self.logger.info(f\\\"Connection closed during VAD monitoring for {client_address}\\\")\\n                    break\\n                except Exception as e:\\n                    self.logger.error(f\\\"Error in VAD monitoring for {client_address}: {e}\\\")\\n                    break\\n        \\n        except asyncio.CancelledError:\\n            self.logger.info(f\\\"VAD monitoring cancelled for {client_address}\\\")\\n        except Exception as e:\\n            self.logger.error(f\\\"Unexpected error in VAD monitoring for {client_address}: {e}\\\")\\n        finally:\\n            self.logger.info(f\\\"VAD monitoring ended for {client_address}\\\")\\n    \\n    async def _trigger_interruption(self, connection_id: str, websocket):\\n        \\\"\\\"\\\"\\n        Handle interruption detection by stopping AI speech and processing user input.\\n        \\n        Args:\\n            connection_id: Connection identifier\\n            websocket: WebSocket connection\\n        \\\"\\\"\\\"\\n        connection = self.active_connections.get(connection_id)\\n        if not connection:\\n            return\\n        \\n        client_address = connection[\\\"client_address\\\"]\\n        self.logger.info(f\\\"🚨 Triggering interruption for {client_address}\\\")\\n        \\n        # Mark interruption detected\\n        connection[\\\"interruption_detected\\\"] = True\\n        connection[\\\"ai_speaking\\\"] = False\\n        connection[\\\"last_interruption_time\\\"] = asyncio.get_event_loop().time() * 1000\\n        \\n        # Cancel TTS generation\\n        if audio_processor.is_tts_active(connection_id):\\n            audio_processor.cancel_tts(connection_id)\\n            self.logger.info(f\\\"Cancelled TTS generation for {client_address}\\\")\\n        \\n        # Process buffered audio if available\\n        if connection[\\\"audio_buffer\\\"]:\\n            self.logger.info(f\\\"Processing {len(connection['audio_buffer'])} buffered audio chunks for {client_address}\\\")\\n            \\n            # Combine buffered audio chunks\\n            combined_audio = b''.join(connection[\\\"audio_buffer\\\"])\\n            \\n            # Clear buffer\\n            connection[\\\"audio_buffer\\\"] = []\\n            \\n            # Process the interrupted audio as user input\\n            await self._handle_audio_message(connection, combined_audio)\\n    \\n    def _should_process_audio_with_vad(self, audio_bytes: bytes) -> bool:\\n        \\\"\\\"\\\"\\n        Enhanced audio validation that combines existing validation with VAD.\\n        \\n        Args:\\n            audio_bytes: Raw audio data\\n            \\n        Returns:\\n            bool: True if audio should be processed\\n        \\\"\\\"\\\"\\n        # First apply existing validation\\n        if not self._should_process_audio(audio_bytes):\\n            return False\\n        \\n        # If VAD is enabled, also check for speech presence\\n        if VAD_ENABLED and vad_manager.is_vad_available():\\n            is_speech = vad_manager.is_speech_present(audio_bytes)\\n            if VAD_DEBUG_LOGGING:\\n                speech_prob = vad_manager.detect_speech(audio_bytes)\\n                self.logger.debug(f\\\"VAD validation: is_speech={is_speech}, prob={speech_prob:.3f}\\\")\\n            return is_speech\\n        \\n        return True\\n\\n\\n# Global WebSocket handler instance\\nwebsocket_handler = WebSocketHandler()\\n\"}],\"isError\":false}",
            "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
          },
          "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
        },
        {
          "id": "msg-72286",
          "subType": "code_analysis",
          "type": "text",
          "content": "<p>I will review the model_manager and audio processor to see how language detection, ASR, and TTS are configured, which could cause output in unexpected languages during interruptions.</p>",
          "originalContent": "I will review the model_manager and audio processor to see how language detection, ASR, and TTS are configured, which could cause output in unexpected languages during interruptions.",
          "isTyping": true
        },
        {
          "id": "call_5DmQuU8nSFvD17KMrFG4abhF",
          "type": "tool_use",
          "subType": "tool_use",
          "isTyping": false,
          "toolUseData": {
            "reasoning": " ",
            "tool": "read_file",
            "serverName": "File System",
            "args": "{\"path\":\"/home/novel/server/models.py\"}",
            "identifier": "call_5DmQuU8nSFvD17KMrFG4abhF",
            "pendingApproval": false,
            "serverType": "BUILTIN",
            "isError": false,
            "message": "{\"content\":[{\"type\":\"text\",\"text\":\"\\\"\\\"\\\"\\nmodels.py\\nAI model management module for Ultravox and Piper TTS.\\nHandles model loading, initialization, and inference operations.\\n\\\"\\\"\\\"\\n\\nimport os\\nimport logging\\nimport asyncio\\nfrom typing import Optional, Any, Dict, List\\nfrom concurrent.futures import ThreadPoolExecutor\\n\\ntry:\\n    import transformers\\n    import torch\\n    from piper import PiperVoice\\n    import librosa\\n    import numpy as np\\nexcept ImportError as e:\\n    print(f\\\"Warning: Some dependencies not available: {e}\\\")\\n    transformers = None\\n    torch = None\\n    PiperVoice = None\\n    librosa = None\\n    np = None\\n\\nfrom config import (\\n    DEVICE, ULTRAVOX_MODEL, PIPER_MODEL_NAME, PIPER_ONNX_FILE, \\n    PIPER_JSON_FILE, MAX_WORKERS, THREAD_NAME_PREFIX\\n)\\nfrom prompt_logger import prompt_logger\\n\\n\\ndef sanitize_audio_placeholders(text: str) -> str:\\n    \\\"\\\"\\\"Remove ALL audio placeholders from text\\\"\\\"\\\"\\n    if not text:\\n        return text\\n    \\n    import re\\n    # Remove all variations of audio placeholders\\n    text = re.sub(r'<\\\\|audio\\\\|?>', '', text)\\n    text = re.sub(r'<\\\\|audio\\\\|?', '', text)\\n    text = re.sub(r'<\\\\|audio', '', text)\\n    \\n    # Clean up multiple spaces\\n    text = re.sub(r'\\\\s+', ' ', text)\\n    \\n    return text.strip()\\n\\n\\ndef ensure_one_audio_placeholder_last_user(turns: List[Dict[str, str]]) -> List[Dict[str, str]]:\\n    \\\"\\\"\\\"\\n    FIXED VERSION: Ensures exactly ONE <|audio|> placeholder in last USER turn.\\n    \\n    Critical fix: Must handle conversations with MULTIPLE turns correctly.\\n    Previous logic was breaking on longer conversations (5+ turns).\\n    \\\"\\\"\\\"\\n    if not turns:\\n        return turns\\n    \\n    logger = logging.getLogger(__name__)\\n    \\n    # STEP 1: Clean ALL audio placeholders from ALL turns (critical!)\\n    for i, turn in enumerate(turns):\\n        if \\\"content\\\" in turn:\\n            original_content = turn[\\\"content\\\"]\\n            turn[\\\"content\\\"] = sanitize_audio_placeholders(turn[\\\"content\\\"])\\n            \\n            if original_content != turn[\\\"content\\\"]:\\n                logger.debug(f\\\"Turn {i}: Removed audio placeholders\\\")\\n    \\n    # STEP 2: Verify all placeholders are gone\\n    total_markers = sum(t.get(\\\"content\\\", \\\"\\\").count(\\\"<|audio|>\\\") for t in turns)\\n    if total_markers > 0:\\n        logger.error(f\\\"CRITICAL: After sanitization, still found {total_markers} markers!\\\")\\n        # This shouldn't happen, but if it does, log for debugging\\n        for i, turn in enumerate(turns):\\n            content = turn.get(\\\"content\\\", \\\"\\\")\\n            if \\\"<|audio|>\\\" in content:\\n                logger.error(f\\\"Turn {i} still has marker: {content[:100]}\\\")\\n    \\n    # STEP 3: Find the LAST user turn (not first, not last assistant)\\n    last_user_idx = -1\\n    for i in range(len(turns) - 1, -1, -1):\\n        if turns[i].get(\\\"role\\\") == \\\"user\\\":\\n            last_user_idx = i\\n            break\\n    \\n    logger.debug(f\\\"Found last user turn at index: {last_user_idx}\\\")\\n    \\n    # STEP 4: Handle case where no user turns exist\\n    if last_user_idx == -1:\\n        logger.warning(\\\"No user turns found - adding new user turn\\\")\\n        turns.append({\\n            \\\"role\\\": \\\"user\\\",\\n            \\\"content\\\": \\\"[Audio input] <|audio|>\\\"\\n        })\\n        return turns\\n    \\n    # STEP 5: Add ONE placeholder to last user turn\\n    last_user_content = turns[last_user_idx].get(\\\"content\\\", \\\"\\\").strip()\\n    if not last_user_content:\\n        last_user_content = \\\"[Audio input]\\\"\\n    \\n    # ADD placeholder to last user turn (NOT after, but AS PART OF the content)\\n    turns[last_user_idx][\\\"content\\\"] = last_user_content + \\\" <|audio|>\\\"\\n    \\n    logger.debug(f\\\"Added placeholder to user turn {last_user_idx}: '{turns[last_user_idx]['content']}'\\\")\\n    \\n    # STEP 6: Final verification - must have EXACTLY 1 marker\\n    total_markers = sum(t.get(\\\"content\\\", \\\"\\\").count(\\\"<|audio|>\\\") for t in turns)\\n    logger.info(f\\\"Final marker count: {total_markers}\\\")\\n    \\n    if total_markers != 1:\\n        logger.error(f\\\"FATAL: Expected 1 marker, got {total_markers}!\\\")\\n        logger.error(\\\"Conversation structure:\\\")\\n        for i, turn in enumerate(turns):\\n            role = turn.get(\\\"role\\\", \\\"?\\\")\\n            content = turn.get(\\\"content\\\", \\\"\\\")[:60]\\n            markers = content.count(\\\"<|audio|>\\\")\\n            logger.error(f\\\"  Turn {i}: role={role}, markers={markers}, content='{content}...'\\\")\\n        \\n        # EMERGENCY FALLBACK: Rebuild from scratch\\n        logger.warning(\\\"EMERGENCY FALLBACK: Rebuilding conversation structure\\\")\\n        # Keep only system prompt and user content, rebuild\\n        new_turns = []\\n        for turn in turns:\\n            if turn.get(\\\"role\\\") == \\\"system\\\":\\n                new_turns.append(turn)\\n        \\n        # Add user/assistant alternating without markers\\n        for turn in turns:\\n            if turn.get(\\\"role\\\") in [\\\"user\\\", \\\"assistant\\\"]:\\n                new_turns.append(turn)\\n        \\n        # Add placeholder to last user\\n        for i in range(len(new_turns) - 1, -1, -1):\\n            if new_turns[i].get(\\\"role\\\") == \\\"user\\\":\\n                content = new_turns[i].get(\\\"content\\\", \\\"\\\").strip()\\n                new_turns[i][\\\"content\\\"] = content + \\\" <|audio|>\\\"\\n                break\\n        \\n        final_markers = sum(t.get(\\\"content\\\", \\\"\\\").count(\\\"<|audio|>\\\") for t in new_turns)\\n        logger.warning(f\\\"After emergency rebuild: {final_markers} markers\\\")\\n        \\n        if final_markers == 1:\\n            logger.warning(\\\"Emergency rebuild successful!\\\")\\n            return new_turns\\n        else:\\n            logger.error(\\\"Emergency rebuild FAILED!\\\")\\n            # Last resort: return error to caller\\n            return turns\\n    \\n    return turns\\n\\n\\nclass ModelManager:\\n    \\\"\\\"\\\"Manages AI models and their inference operations.\\\"\\\"\\\"\\n    \\n    def __init__(self):\\n        self.logger = logging.getLogger(__name__)\\n        self.ultravox_pipeline = None\\n        self.piper_voice = None\\n        self.whisper_processor = None\\n        self.whisper_model = None\\n        self.executor = ThreadPoolExecutor(\\n            max_workers=MAX_WORKERS, \\n            thread_name_prefix=THREAD_NAME_PREFIX\\n        )\\n        self._load_models()\\n    \\n    def _load_models(self):\\n        \\\"\\\"\\\"Load all required AI models.\\\"\\\"\\\"\\n        self._load_ultravox()\\n        self._load_piper()\\n        self._load_whisper()\\n    \\n    def _load_ultravox(self):\\n        \\\"\\\"\\\"Load the Ultravox pipeline.\\\"\\\"\\\"\\n        if transformers is None or torch is None:\\n            self.logger.error(\\\"Transformers or PyTorch not available\\\")\\n            raise RuntimeError(\\\"Required dependencies not available\\\")\\n            \\n        self.logger.info(\\\"Loading Ultravox pipeline...\\\")\\n        try:\\n            self.ultravox_pipeline = transformers.pipeline(\\n                model=ULTRAVOX_MODEL,\\n                trust_remote_code=True,\\n                device=DEVICE,\\n                torch_dtype=torch.float16 if DEVICE == \\\"cuda\\\" else torch.float32\\n            )\\n            self.logger.info(f\\\"Ultravox pipeline loaded successfully on {DEVICE}!\\\")\\n            if DEVICE == \\\"cuda\\\":\\n                self.logger.info(\\\"Model is using GPU acceleration for faster inference\\\")\\n        except Exception as e:\\n            self.logger.error(f\\\"Failed to load Ultravox pipeline: {e}\\\")\\n            raise\\n    \\n    def _load_piper(self):\\n        \\\"\\\"\\\"Load the Piper TTS model.\\\"\\\"\\\"\\n        if PiperVoice is None:\\n            self.logger.warning(\\\"Piper TTS not available\\\")\\n            self.piper_voice = None\\n            return\\n            \\n        self.logger.info(\\\"Loading Piper TTS...\\\")\\n        try:\\n            # Check for required model files\\n            if self._check_piper_files():\\n                self.piper_voice = PiperVoice.load(PIPER_MODEL_NAME)\\n                self.logger.info(\\\"Piper TTS loaded successfully!\\\")\\n            else:\\n                self.logger.warning(\\\"TTS functionality will be disabled\\\")\\n                self.piper_voice = None\\n        except Exception as e:\\n            self.logger.error(f\\\"Failed to load Piper TTS: {e}\\\")\\n            self.logger.warning(\\\"TTS functionality will be disabled\\\")\\n            self.piper_voice = None\\n    \\n    def _load_whisper(self):\\n        \\\"\\\"\\\"Load the Whisper model for speech-to-text transcription.\\\"\\\"\\\"\\n        if transformers is None or torch is None:\\n            self.logger.error(\\\"Transformers or PyTorch not available for Whisper\\\")\\n            self.whisper_processor = None\\n            self.whisper_model = None\\n            return\\n            \\n        self.logger.info(\\\"Loading Whisper model...\\\")\\n        try:\\n            from transformers import WhisperProcessor, WhisperForConditionalGeneration\\n            \\n            self.whisper_processor = WhisperProcessor.from_pretrained(\\\"openai/whisper-small\\\")\\n            self.whisper_model = WhisperForConditionalGeneration.from_pretrained(\\\"openai/whisper-small\\\")\\n            \\n            # Move model to device\\n            self.whisper_model = self.whisper_model.to(DEVICE)\\n            \\n            self.logger.info(f\\\"Whisper model loaded successfully on {DEVICE}!\\\")\\n            if DEVICE == \\\"cuda\\\":\\n                self.logger.info(\\\"Whisper model is using GPU acceleration for faster inference\\\")\\n        except Exception as e:\\n            self.logger.error(f\\\"Failed to load Whisper model: {e}\\\")\\n            self.logger.warning(\\\"Whisper transcription functionality will be disabled\\\")\\n            self.whisper_processor = None\\n            self.whisper_model = None\\n    \\n    def _check_piper_files(self) -> bool:\\n        \\\"\\\"\\\"Check if all required Piper model files exist.\\\"\\\"\\\"\\n        required_files = [PIPER_ONNX_FILE, PIPER_JSON_FILE, PIPER_MODEL_NAME]\\n        missing_files = []\\n        \\n        for file_path in required_files:\\n            if not os.path.exists(file_path):\\n                missing_files.append(file_path)\\n        \\n        if missing_files:\\n            self.logger.warning(f\\\"Missing required files: {missing_files}\\\")\\n            return False\\n        \\n        self.logger.info(f\\\"Found ONNX model file: {PIPER_ONNX_FILE}\\\")\\n        self.logger.info(f\\\"Found JSON config file: {PIPER_JSON_FILE}\\\")\\n        self.logger.info(f\\\"Found model symlink: {PIPER_MODEL_NAME}\\\")\\n        return True\\n    \\n    async def process_audio(self, audio_data: bytes, conversation_turns: List[Dict[str, str]], connection_id: str = None) -> str:\\n        \\\"\\\"\\\"Process audio input using Ultravox pipeline with adaptive validation.\\\"\\\"\\\"\\n        if not self.ultravox_pipeline:\\n            raise RuntimeError(\\\"Ultravox pipeline not loaded\\\")\\n        \\n        try:\\n            import librosa\\n            import io\\n            from utils import is_valid_speech_audio\\n            from config import ULTRAVOX_MIN_ENERGY, ULTRAVOX_MIN_SPEECH_RATIO\\n            \\n            # Load audio\\n            audio_stream = io.BytesIO(audio_data)\\n            audio, sr = librosa.load(audio_stream, sr=16000)\\n            self.logger.info(f\\\"Loaded audio: {len(audio)} samples at {sr}Hz\\\")\\n            \\n            if len(audio) == 0:\\n                return \\\"[AUDIO_EMPTY]\\\"\\n            \\n            # TIER 2 VALIDATION\\n            is_valid_audio, stats = is_valid_speech_audio(\\n                audio_data,\\n                min_energy_threshold=ULTRAVOX_MIN_ENERGY,\\n                min_speech_ratio=ULTRAVOX_MIN_SPEECH_RATIO\\n            )\\n            \\n            if not is_valid_audio:\\n                reason = stats.get('reason', 'unknown')\\n                self.logger.warning(f\\\"[ULTRAVOX] Audio rejected: {reason} (energy={stats.get('rms_energy', 0):.1f}, speech_ratio={stats.get('speech_ratio', 0):.3f})\\\")\\n                \\n                # CRITICAL FIX: Clean up any existing audio placeholders from conversation context\\n                # This prevents accumulation of placeholders when audio is repeatedly rejected\\n                self._cleanup_audio_placeholders(conversation_turns)\\n                \\n                return \\\"[AUDIO_REJECTED_BY_ULTRAVOX_VALIDATION]\\\"\\n            \\n            # FIX: Reset conversation_turns to be a mutable copy\\n            conversation_turns = list(conversation_turns)  # Make copy\\n            \\n            # Check if conversation_turns already has proper structure (from get_conversation_context_for_ai)\\n            # If so, we need to add audio placeholder to the last user turn\\n            has_audio_placeholders = any(\\\"<|audio|>\\\" in turn.get(\\\"content\\\", \\\"\\\") for turn in conversation_turns)\\n            \\n            if not has_audio_placeholders:\\n                # This is processed context from get_conversation_context_for_ai, add a NEW user turn with placeholder\\n                # We should always add a new user turn for the current audio input, not modify existing turns\\n                conversation_turns.append({\\n                    \\\"role\\\": \\\"user\\\",\\n                    \\\"content\\\": \\\"[Audio input] <|audio|>\\\"\\n                })\\n                self.logger.info(\\\"Added new user turn with audio placeholder to processed context\\\")\\n            else:\\n                # This is raw conversation data, use the existing function\\n                conversation_turns = ensure_one_audio_placeholder_last_user(conversation_turns)\\n            \\n            # Verify exactly 1 placeholder before sending to Ultravox\\n            total_audio_markers = sum(turn.get(\\\"content\\\", \\\"\\\").count(\\\"<|audio|>\\\") for turn in conversation_turns)\\n            \\n            if total_audio_markers != 1:\\n                self.logger.error(f\\\"CRITICAL: Audio placeholder mismatch! Expected 1, found {total_audio_markers}\\\")\\n                self.logger.error(\\\"Turn structure before sending:\\\")\\n                for idx, turn in enumerate(conversation_turns):\\n                    role = turn.get(\\\"role\\\", \\\"?\\\")\\n                    content = turn.get(\\\"content\\\", \\\"\\\")\\n                    markers = content.count(\\\"<|audio|>\\\")\\n                    self.logger.error(f\\\"  Turn {idx}: role={role}, markers={markers}, content='{content[:80]}'\\\")\\n                \\n                # CRITICAL FIX: Clean up placeholders and try to fix the issue\\n                self.logger.warning(\\\"Attempting to fix audio placeholder mismatch by cleaning up...\\\")\\n                self._cleanup_audio_placeholders(conversation_turns)\\n                \\n                # Re-add exactly one placeholder by adding a new user turn\\n                conversation_turns.append({\\n                    \\\"role\\\": \\\"user\\\",\\n                    \\\"content\\\": \\\"[Audio input] <|audio|>\\\"\\n                })\\n                self.logger.info(\\\"Fixed: Added new user turn with single audio placeholder\\\")\\n                \\n                # Verify fix\\n                total_audio_markers = sum(turn.get(\\\"content\\\", \\\"\\\").count(\\\"<|audio|>\\\") for turn in conversation_turns)\\n                if total_audio_markers != 1:\\n                    self.logger.error(f\\\"Failed to fix audio placeholder issue. Still have {total_audio_markers} placeholders\\\")\\n                    return \\\"[PLACEHOLDER_VALIDATION_FAILED]\\\"\\n                else:\\n                    self.logger.info(\\\"✅ Successfully fixed audio placeholder issue\\\")\\n            \\n            # Ensure system prompt\\n            if not conversation_turns or conversation_turns[0].get(\\\"role\\\") != \\\"system\\\":\\n                system_prompt = {\\n                    \\\"role\\\": \\\"system\\\",\\n                    \\\"content\\\": \\\"You are Alexa, an HR recruiter from Novel Office calling Business Development Manager applicants. Speak naturally and professionally, as in a real phone call. Keep responses short, 1–2 sentences at a time. Do not use lists, bullets, emojis, stage directions, or overly formal prose; this is a live voice conversation. Always maintain your identity as Alexa and address the candidate appropriately.\\\"\\n                }\\n                conversation_turns = [system_prompt] + conversation_turns\\n            \\n            self.logger.info(f\\\"[ULTRAVOX] Sending {len(conversation_turns)} turns to pipeline\\\")\\n            for idx, turn in enumerate(conversation_turns):\\n                role = turn.get(\\\"role\\\", \\\"?\\\")\\n                content = turn.get(\\\"content\\\", \\\"\\\")\\n                markers = content.count(\\\"<|audio|>\\\")\\n                self.logger.info(f\\\"  Turn {idx}: role={role}, markers={markers}, content='{content[:70]}'\\\")\\n            \\n            if connection_id:\\n                prompt_logger.log_ultravox_prompt(connection_id, audio_data, conversation_turns)\\n            \\n            # Run inference in thread executor\\n            loop = asyncio.get_event_loop()\\n            result = await loop.run_in_executor(\\n                self.executor,\\n                lambda: self.ultravox_pipeline({\\n                    'audio': audio, \\n                    'turns': conversation_turns, \\n                    'sampling_rate': sr\\n                }, max_new_tokens=2000, do_sample=True, temperature=0.7)\\n            )\\n            \\n            self.logger.info(\\\"Ultravox inference completed successfully\\\")\\n            response_text = self._extract_response_text(result)\\n            \\n            if connection_id:\\n                prompt_logger.log_ultravox_prompt(\\n                    connection_id, audio_data, conversation_turns, response_text\\n                )\\n            \\n            return response_text\\n            \\n        except Exception as e:\\n            self.logger.error(f\\\"Ultravox pipeline error: {e}\\\")\\n            import traceback\\n            self.logger.error(f\\\"Traceback: {traceback.format_exc()}\\\")\\n            return f\\\"[ULTRAVOX_ERROR: {str(e)}]\\\"\\n    \\n    def _extract_response_text(self, result: Any) -> str:\\n        \\\"\\\"\\\"Extract text response from Ultravox pipeline result.\\\"\\\"\\\"\\n        if isinstance(result, list) and len(result) > 0:\\n            if isinstance(result[0], dict) and 'generated_text' in result[0]:\\n                return result[0]['generated_text']\\n            elif isinstance(result[0], str):\\n                return result[0]\\n            else:\\n                return str(result[0])\\n        elif isinstance(result, dict):\\n            if 'generated_text' in result:\\n                return result['generated_text']\\n            elif 'text' in result:\\n                return result['text']\\n            else:\\n                return str(result)\\n        elif isinstance(result, str):\\n            return result\\n        else:\\n            return str(result)\\n    \\n    def _is_conversational_response(self, text: str) -> bool:\\n        \\\"\\\"\\\"Check if the response is conversational rather than a pure transcription.\\\"\\\"\\\"\\n        if not text:\\n            return False\\n        \\n        # Common conversational phrases that indicate AI responses\\n        conversational_indicators = [\\n            \\\"I can't\\\", \\\"I cannot\\\", \\\"I'm not able to\\\", \\\"I don't\\\", \\\"I won't\\\",\\n            \\\"I can help you\\\", \\\"I understand\\\", \\\"I see\\\", \\\"Let me\\\", \\\"I'll\\\",\\n            \\\"I'm sorry\\\", \\\"I apologize\\\", \\\"I'm afraid\\\", \\\"I'm doing well\\\",\\n            \\\"Thank you for asking\\\", \\\"I'm ready when you are\\\",\\n            \\\"I'm not able to transcribe\\\", \\\"I can't provide guidance\\\",\\n            \\\"I cannot provide information\\\", \\\"I can't give you any information\\\"\\n        ]\\n        \\n        text_lower = text.lower().strip()\\n        \\n        # Check if it starts with conversational indicators\\n        for indicator in conversational_indicators:\\n            if text_lower.startswith(indicator.lower()):\\n                return True\\n        \\n        # Check if it's a very long response (transcriptions should be relatively short)\\n        if len(text) > 200:\\n            return True\\n            \\n        # Check if it contains multiple sentences with conversational structure\\n        sentences = text.split('.')\\n        if len(sentences) > 3:\\n            return True\\n            \\n        return False\\n    \\n    def _extract_transcription_from_response(self, response: str) -> str:\\n        \\\"\\\"\\\"Try to extract actual transcription from a conversational response.\\\"\\\"\\\"\\n        if not response:\\n            return response\\n        \\n        # Look for quoted text that might be the actual transcription\\n        import re\\n        \\n        # Look for text in quotes\\n        quoted_matches = re.findall(r'\\\"([^\\\"]*)\\\"', response)\\n        if quoted_matches:\\n            # Return the longest quoted text (likely the transcription)\\n            return max(quoted_matches, key=len)\\n        \\n        # Look for text after common transcription indicators\\n        transcription_indicators = [\\n            \\\"the audio says:\\\", \\\"the transcription is:\\\", \\\"the text is:\\\",\\n            \\\"transcribed as:\\\", \\\"says:\\\", \\\"audio contains:\\\"\\n        ]\\n        \\n        text_lower = response.lower()\\n        for indicator in transcription_indicators:\\n            if indicator in text_lower:\\n                # Extract text after the indicator\\n                parts = response.split(indicator, 1)\\n                if len(parts) > 1:\\n                    extracted = parts[1].strip()\\n                    # Remove any trailing conversational text\\n                    sentences = extracted.split('.')\\n                    if sentences:\\n                        return sentences[0].strip()\\n        \\n        # If no clear transcription found, return the original response\\n        # but truncate if it's too long\\n        if len(response) > 100:\\n            return response[:100] + \\\"...\\\"\\n        \\n        return response\\n    \\n    async def transcribe_audio(self, audio_data: bytes, connection_id: str = None) -> str:\\n        \\\"\\\"\\\"Transcribe audio input to text using Whisper model.\\\"\\\"\\\"\\n        if not self.whisper_processor or not self.whisper_model:\\n            raise RuntimeError(\\\"Whisper model not loaded\\\")\\n        \\n        try:\\n            import io\\n            \\n            # Load audio from bytes\\n            audio_stream = io.BytesIO(audio_data)\\n            audio, sr = librosa.load(audio_stream, sr=16000)\\n            self.logger.info(f\\\"Loaded audio for transcription: {len(audio)} samples at {sr}Hz\\\")\\n            \\n            if len(audio) == 0:\\n                return \\\"Error: Audio file contains no data\\\"\\n            \\n            # Log the prompt being sent to Whisper\\n            if connection_id:\\n                prompt_logger.log_whisper_prompt(\\n                    connection_id=connection_id,\\n                    audio_data=audio_data\\n                )\\n            \\n            # Calculate duration and determine if we need chunking\\n            duration = len(audio) / sr\\n            chunk_length = 30  # 30 seconds per chunk\\n            \\n            # If audio is short, process normally\\n            if duration <= chunk_length:\\n                result = await self._transcribe_audio_chunk(audio, sr)\\n            else:\\n                # For longer audio, process in chunks\\n                self.logger.info(f\\\"Processing audio in {chunk_length}-second chunks...\\\")\\n                chunk_samples = chunk_length * sr\\n                full_transcription = []\\n                \\n                for i in range(0, len(audio), chunk_samples):\\n                    chunk = audio[i:i + chunk_samples]\\n                    if len(chunk) < sr:  # Skip very short chunks\\n                        break\\n                        \\n                    self.logger.info(f\\\"Processing chunk {i//chunk_samples + 1}/{(len(audio) + chunk_samples - 1)//chunk_samples}\\\")\\n                    \\n                    # Process chunk\\n                    chunk_transcription = await self._transcribe_audio_chunk(chunk, sr)\\n                    if chunk_transcription.strip():\\n                        full_transcription.append(chunk_transcription)\\n                \\n                # Combine all chunks\\n                result = \\\" \\\".join(full_transcription)\\n            \\n            # Log the response\\n            if connection_id:\\n                prompt_logger.log_whisper_prompt(\\n                    connection_id=connection_id,\\n                    audio_data=audio_data,\\n                    response=result\\n                )\\n            \\n            self.logger.info(f\\\"Final transcription result: {result[:100]}...\\\")\\n            return result\\n            \\n        except Exception as e:\\n            self.logger.error(f\\\"Audio transcription error: {e}\\\")\\n            return f\\\"Error transcribing audio: {str(e)}\\\"\\n    \\n    async def _transcribe_audio_chunk(self, audio: np.ndarray, sr: int) -> str:\\n        \\\"\\\"\\\"Transcribe a single audio chunk using Whisper.\\\"\\\"\\\"\\n        try:\\n            # Process audio\\n            input_features = self.whisper_processor(audio, sampling_rate=sr, return_tensors=\\\"pt\\\").input_features\\n            \\n            # Move input features to the same device as the model\\n            input_features = input_features.to(DEVICE)\\n            \\n            # Generate transcription\\n            with torch.no_grad():  # Save memory\\n                predicted_ids = self.whisper_model.generate(input_features, max_length=448)\\n            \\n            # Decode to text\\n            transcription = self.whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)\\n            return transcription[0]\\n            \\n        except Exception as e:\\n            self.logger.error(f\\\"Error transcribing audio chunk: {e}\\\")\\n            return \\\"\\\"\\n    \\n    async def process_text_with_context(self, user_text: str, conversation_turns: List[Dict[str, str]], conversation_history: List[Dict[str, str]]) -> str:\\n        \\\"\\\"\\\"Process text input with conversation context using Ultravox pipeline.\\\"\\\"\\\"\\n        if not self.ultravox_pipeline:\\n            raise RuntimeError(\\\"Ultravox pipeline not loaded\\\")\\n        \\n        try:\\n            # Combine conversation turns with history\\n            combined_turns = conversation_turns.copy()\\n            \\n            # Add conversation history\\n            for entry in conversation_history:\\n                combined_turns.append(entry)\\n            \\n            # Add the current user input\\n            combined_turns.append({\\\"role\\\": \\\"user\\\", \\\"content\\\": user_text})\\n            \\n            # Ensure proper audio placeholder format\\n            combined_turns = ensure_one_audio_placeholder_last_user(combined_turns)\\n            \\n            # Ensure system prompt is always first and consistent\\n            if not combined_turns or combined_turns[0].get(\\\"role\\\") != \\\"system\\\":\\n                # Add system prompt if missing\\n                system_prompt = {\\n                    \\\"role\\\": \\\"system\\\",\\n                    \\\"content\\\": \\\"\\\"\\\"You are Alexa, an HR recruiter from Novel Office calling Business Development Manager applicants. Speak naturally and professionally, as in a real phone call. Keep responses short, 1–2 sentences at a time. Do not use lists, bullets, emojis, stage directions, or overly formal prose; this is a live voice conversation. Always maintain your identity as Alexa and address the candidate appropriately.\\\"\\\"\\\"\\n                }\\n                combined_turns = [system_prompt] + combined_turns\\n            \\n            # Create a simple audio placeholder (silence) since we're processing text\\n            import numpy as np\\n            silence_audio = np.zeros(1600, dtype=np.float32)  # 0.1 seconds of silence at 16kHz\\n            \\n            # Run inference in thread executor\\n            loop = asyncio.get_event_loop()\\n            result = await loop.run_in_executor(\\n                self.executor,\\n                lambda: self.ultravox_pipeline({\\n                    'audio': silence_audio, \\n                    'turns': combined_turns, \\n                    'sampling_rate': 16000\\n                }, max_new_tokens=2000, do_sample=True, temperature=0.7)\\n            )\\n            \\n            self.logger.info(\\\"Text processing with context completed\\\")\\n            response_text = self._extract_response_text(result)\\n            # Sanitize any audio placeholders from the response\\n            return sanitize_audio_placeholders(response_text)\\n            \\n        except Exception as e:\\n            self.logger.error(f\\\"Text processing error: {e}\\\")\\n            return f\\\"Error processing text: {str(e)}\\\"\\n    \\n    def is_tts_available(self) -> bool:\\n        \\\"\\\"\\\"Check if TTS functionality is available.\\\"\\\"\\\"\\n        return self.piper_voice is not None\\n    \\n    async def summarize_conversation(self, conversation_history: List[dict], connection_id: str = None) -> str:\\n        \\\"\\\"\\\"Run the Ultravox/Llama model to summarize the conversation.\\\"\\\"\\\"\\n        if not self.ultravox_pipeline:\\n            raise RuntimeError(\\\"Ultravox pipeline not loaded\\\")\\n\\n        # Defensive checks\\n        if not isinstance(conversation_history, list):\\n            raise ValueError(f\\\"Conversation history must be a list, got {type(conversation_history)}\\\")\\n        \\n        if not conversation_history:\\n            return \\\"No conversation history available for summarization.\\\"\\n        \\n        for i, entry in enumerate(conversation_history):\\n            if not isinstance(entry, dict):\\n                raise ValueError(f\\\"Entry {i} must be a dict, got {type(entry)}: {entry}\\\")\\n            if \\\"type\\\" not in entry:\\n                raise ValueError(f\\\"Entry {i} missing 'type' field: {entry}\\\")\\n\\n        # Format for LLM using the proper pipeline format, sanitizing any audio markers\\n        turns = []\\n        for entry in conversation_history:\\n            if entry[\\\"type\\\"] == \\\"user\\\":\\n                # Remove any <|audio|> from user text\\n                user_text = sanitize_audio_placeholders(entry['User'])\\n                turns.append({\\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Candidate: {user_text}\\\"})\\n            elif entry[\\\"type\\\"] == \\\"ai\\\":\\n                ai_text = sanitize_audio_placeholders(entry['AI'])\\n                turns.append({\\\"role\\\": \\\"assistant\\\", \\\"content\\\": f\\\"Recruiter: {ai_text}\\\"})\\n\\n        # Add system prompt for summarization (sanitized)\\n        system_prompt = {\\n            \\\"role\\\": \\\"system\\\", \\n            \\\"content\\\": sanitize_audio_placeholders(\\n                \\\"You are a CRM assistant. Summarize the following recruiter-candidate interview in 5-7 lines for CRM notes. Mention candidate's background, key details, interviewer remarks, outcome, and candidate's interest/fit. Be concise and professional.\\\"\\n            )\\n        }\\n        \\n        # Add final user prompt requesting summary (sanitized)\\n        summary_request = {\\n            \\\"role\\\": \\\"user\\\",\\n            \\\"content\\\": sanitize_audio_placeholders(\\\"Please provide a summary of this interview conversation.\\\")\\n        }\\n        \\n        # Combine all turns\\n        all_turns = [system_prompt] + turns + [summary_request]\\n        \\n        # Ensure proper audio placeholder format for Ultravox pipeline\\n        all_turns = ensure_one_audio_placeholder_last_user(all_turns)\\n        \\n        # Log the prompt being sent for summarization\\n        if connection_id:\\n            prompt_logger.log_summarization_prompt(\\n                connection_id=connection_id,\\n                conversation_history=conversation_history\\n            )\\n        \\n        # Create silence audio for the pipeline (since we're doing text-only summarization)\\n        import numpy as np\\n        silence_audio = np.zeros(1600, dtype=np.float32)  # 0.1 seconds of silence at 16kHz\\n\\n        # Send to LLM for summarization using proper pipeline format\\n        loop = asyncio.get_event_loop()\\n        result = await loop.run_in_executor(\\n            self.executor,\\n            lambda: self.ultravox_pipeline({\\n                'audio': silence_audio, \\n                'turns': all_turns, \\n                'sampling_rate': 16000\\n            }, max_new_tokens=350, do_sample=True, temperature=0.7)\\n        )\\n        \\n        summary_text = self._extract_response_text(result).strip()\\n        \\n        # Log the response\\n        if connection_id:\\n            prompt_logger.log_summarization_prompt(\\n                connection_id=connection_id,\\n                conversation_history=conversation_history,\\n                response=summary_text\\n            )\\n        \\n        return summary_text\\n\\n    def _cleanup_audio_placeholders(self, conversation_turns: List[Dict[str, str]]) -> None:\\n        \\\"\\\"\\\"\\n        Clean up audio placeholders from conversation turns to prevent accumulation\\n        when audio is rejected by validation.\\n        \\n        Args:\\n            conversation_turns: List of conversation turns to clean up\\n        \\\"\\\"\\\"\\n        if not conversation_turns:\\n            return\\n            \\n        self.logger.info(\\\"Cleaning up audio placeholders from conversation context\\\")\\n        \\n        for turn in conversation_turns:\\n            content = turn.get(\\\"content\\\", \\\"\\\")\\n            if \\\"<|audio|>\\\" in content:\\n                # Remove all audio placeholders from this turn\\n                cleaned_content = content.replace(\\\"<|audio|>\\\", \\\"\\\").strip()\\n                \\n                # If content becomes empty after cleanup, set a default\\n                if not cleaned_content:\\n                    if turn.get(\\\"role\\\") == \\\"user\\\":\\n                        cleaned_content = \\\"[Audio input]\\\"\\n                    else:\\n                        cleaned_content = \\\"[Response]\\\"\\n                \\n                turn[\\\"content\\\"] = cleaned_content\\n                self.logger.info(f\\\"Cleaned audio placeholder from {turn.get('role', 'unknown')} turn: '{content[:50]}...' -> '{cleaned_content[:50]}...'\\\")\\n        \\n        # Verify cleanup\\n        total_placeholders = sum(turn.get(\\\"content\\\", \\\"\\\").count(\\\"<|audio|>\\\") for turn in conversation_turns)\\n        if total_placeholders == 0:\\n            self.logger.info(\\\"✅ Successfully cleaned all audio placeholders\\\")\\n        else:\\n            self.logger.warning(f\\\"⚠️ Still found {total_placeholders} audio placeholders after cleanup\\\")\\n\\n    def shutdown(self):\\n        \\\"\\\"\\\"Shutdown the model manager and clean up resources.\\\"\\\"\\\"\\n        self.logger.info(\\\"Shutting down thread executor...\\\")\\n        self.executor.shutdown(wait=True)\\n\\n\\n# Global model manager instance\\nmodel_manager = ModelManager()\\n\"}],\"isError\":false}",
            "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
          },
          "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
        },
        {
          "id": "msg-793057",
          "subType": "code_analysis",
          "type": "text",
          "content": "<p>I will read server/audio_processor.py to see how TTS is handled and whether language or voice settings could cause multilingual output when interrupted.</p>",
          "originalContent": "I will read server/audio_processor.py to see how TTS is handled and whether language or voice settings could cause multilingual output when interrupted.",
          "isTyping": true
        },
        {
          "id": "call_Z1u1sGYTHvizJoKLExcJAGPp",
          "type": "tool_use",
          "subType": "tool_use",
          "isTyping": false,
          "toolUseData": {
            "reasoning": " ",
            "tool": "read_file",
            "serverName": "File System",
            "args": "{\"path\":\"/home/novel/server/audio_processor.py\"}",
            "identifier": "call_Z1u1sGYTHvizJoKLExcJAGPp",
            "pendingApproval": false,
            "serverType": "BUILTIN",
            "isError": false,
            "message": "{\"content\":[{\"type\":\"text\",\"text\":\"\\\"\\\"\\\"\\nAudio processing module for TTS generation and audio handling.\\nHandles text-to-speech conversion and audio streaming.\\n\\\"\\\"\\\"\\n\\nimport io\\nimport wave\\nimport logging\\nimport asyncio\\nfrom typing import Optional, Tuple, Dict, Any\\n\\ntry:\\n    from piper import PiperVoice\\nexcept ImportError as e:\\n    print(f\\\"Warning: Piper TTS not available: {e}\\\")\\n    PiperVoice = None\\n\\nfrom config import TTS_SAMPLE_RATE, MAX_TTS_TEXT_LENGTH\\nfrom utils import validate_text_for_tts, safe_send_response\\nfrom models import model_manager\\n\\n\\nclass AudioProcessor:\\n    \\\"\\\"\\\"Handles audio processing operations including TTS generation.\\\"\\\"\\\"\\n    \\n    def __init__(self):\\n        self.logger = logging.getLogger(__name__)\\n        self.active_tts_tasks: Dict[str, asyncio.Task] = {}  # Track active TTS tasks by connection_id\\n    \\n    async def generate_tts_audio(self, text: str, connection_id: str = None) -> Optional[bytes]:\\n        \\\"\\\"\\\"Generate TTS audio from text using Piper.\\\"\\\"\\\"\\n        if not model_manager.is_tts_available():\\n            self.logger.warning(\\\"TTS not available, cannot generate audio\\\")\\n            return None\\n        \\n        # Validate and clean input text\\n        text = validate_text_for_tts(text)\\n        if not text:\\n            self.logger.warning(\\\"TTS text is empty or whitespace only. Skipping synthesis.\\\")\\n            return None\\n        \\n        self.logger.info(f\\\"TTS input text validation:\\\")\\n        self.logger.info(f\\\"  - Text type: {type(text)}\\\")\\n        self.logger.info(f\\\"  - Text length: {len(text) if text else 0}\\\")\\n        self.logger.info(f\\\"  - Text repr: {repr(text)}\\\")\\n        \\n        try:\\n            self.logger.info(f\\\"Calling PiperVoice.synthesize with text: {repr(text[:100])}...\\\")\\n            \\n            # Generate audio using Piper TTS\\n            audio_data = model_manager.piper_voice.synthesize(text)\\n            self.logger.info(f\\\"Piper synthesize returned type: {type(audio_data)}\\\")\\n            \\n            # Process audio chunks\\n            audio_bytes = self._process_audio_chunks(audio_data)\\n            if not audio_bytes:\\n                return None\\n            \\n            # Convert to WAV format\\n            wav_bytes = self._convert_to_wav(audio_bytes)\\n            self.logger.info(f\\\"Generated TTS audio: {len(wav_bytes)} bytes\\\")\\n            return wav_bytes\\n            \\n        except Exception as e:\\n            self.logger.error(f\\\"Error generating TTS audio: {e}\\\")\\n            import traceback\\n            self.logger.error(f\\\"Full traceback: {traceback.format_exc()}\\\")\\n            return None\\n    \\n    async def generate_tts_audio_cancellable(self, text: str, connection_id: str) -> Optional[bytes]:\\n        \\\"\\\"\\\"\\n        Generate TTS audio with cancellation support for interruption handling.\\n        \\n        Args:\\n            text: Text to synthesize\\n            connection_id: Connection identifier for task tracking\\n            \\n        Returns:\\n            Generated audio bytes or None if cancelled/error\\n        \\\"\\\"\\\"\\n        if not model_manager.is_tts_available():\\n            self.logger.warning(\\\"TTS not available, cannot generate audio\\\")\\n            return None\\n        \\n        # Validate and clean input text\\n        text = validate_text_for_tts(text)\\n        if not text:\\n            self.logger.warning(\\\"TTS text is empty or whitespace only. Skipping synthesis.\\\")\\n            return None\\n        \\n        # Cancel any existing TTS task for this connection\\n        if connection_id in self.active_tts_tasks:\\n            self.logger.info(f\\\"Cancelling existing TTS task for connection {connection_id}\\\")\\n            self.active_tts_tasks[connection_id].cancel()\\n            try:\\n                await self.active_tts_tasks[connection_id]\\n            except asyncio.CancelledError:\\n                pass\\n            del self.active_tts_tasks[connection_id]\\n        \\n        # Create new cancellable TTS task\\n        tts_task = asyncio.create_task(self._generate_tts_async(text))\\n        self.active_tts_tasks[connection_id] = tts_task\\n        \\n        try:\\n            result = await tts_task\\n            return result\\n        except asyncio.CancelledError:\\n            self.logger.info(f\\\"TTS generation cancelled for connection {connection_id}\\\")\\n            return None\\n        except Exception as e:\\n            self.logger.error(f\\\"Error in cancellable TTS generation: {e}\\\")\\n            return None\\n        finally:\\n            # Clean up task tracking\\n            if connection_id in self.active_tts_tasks:\\n                del self.active_tts_tasks[connection_id]\\n    \\n    async def _generate_tts_async(self, text: str) -> Optional[bytes]:\\n        \\\"\\\"\\\"Async wrapper for TTS generation.\\\"\\\"\\\"\\n        try:\\n            # Generate audio using Piper TTS\\n            audio_data = model_manager.piper_voice.synthesize(text)\\n            \\n            # Process audio chunks\\n            audio_bytes = self._process_audio_chunks(audio_data)\\n            if not audio_bytes:\\n                return None\\n            \\n            # Convert to WAV format\\n            wav_bytes = self._convert_to_wav(audio_bytes)\\n            return wav_bytes\\n            \\n        except Exception as e:\\n            self.logger.error(f\\\"Error in async TTS generation: {e}\\\")\\n            return None\\n    \\n    def cancel_tts(self, connection_id: str) -> bool:\\n        \\\"\\\"\\\"\\n        Cancel ongoing TTS generation for a specific connection.\\n        \\n        Args:\\n            connection_id: Connection identifier\\n            \\n        Returns:\\n            bool: True if task was cancelled, False if no active task\\n        \\\"\\\"\\\"\\n        if connection_id in self.active_tts_tasks:\\n            self.logger.info(f\\\"Cancelling TTS task for connection {connection_id}\\\")\\n            self.active_tts_tasks[connection_id].cancel()\\n            return True\\n        return False\\n    \\n    def is_tts_active(self, connection_id: str) -> bool:\\n        \\\"\\\"\\\"\\n        Check if TTS generation is active for a connection.\\n        \\n        Args:\\n            connection_id: Connection identifier\\n            \\n        Returns:\\n            bool: True if TTS is active\\n        \\\"\\\"\\\"\\n        return connection_id in self.active_tts_tasks\\n    \\n    def is_tts_available(self) -> bool:\\n        \\\"\\\"\\\"\\n        Check if TTS functionality is available.\\n        \\n        Returns:\\n            bool: True if TTS is available\\n        \\\"\\\"\\\"\\n        return model_manager.is_tts_available()\\n    \\n    def _process_audio_chunks(self, audio_data) -> Optional[bytes]:\\n        \\\"\\\"\\\"Process audio chunks from Piper TTS output.\\\"\\\"\\\"\\n        if hasattr(audio_data, '__iter__'):\\n            # Convert generator to list to examine the chunks\\n            audio_chunks = list(audio_data)\\n            self.logger.info(f\\\"Piper synthesize returned {len(audio_chunks)} chunks\\\")\\n            \\n            if len(audio_chunks) > 0:\\n                # Extract audio data from AudioChunk objects\\n                audio_bytes = b\\\"\\\"\\n                for idx, chunk in enumerate(audio_chunks):\\n                    self.logger.info(f\\\"Processing chunk {idx}:\\\")\\n                    \\n                    # Check available attributes\\n                    if hasattr(chunk, 'audio_int16_bytes'):\\n                        chunk_bytes = chunk.audio_int16_bytes\\n                        self.logger.info(f\\\"  - audio_int16_bytes: {len(chunk_bytes)} bytes\\\")\\n                        if len(chunk_bytes) > 0:\\n                            audio_bytes += chunk_bytes\\n                            self.logger.info(f\\\"  - Added {len(chunk_bytes)} bytes to total\\\")\\n                        else:\\n                            self.logger.warning(f\\\"  - audio_int16_bytes is empty!\\\")\\n                    elif hasattr(chunk, 'audio_int16_array'):\\n                        chunk_array = chunk.audio_int16_array\\n                        chunk_bytes = chunk_array.tobytes()\\n                        self.logger.info(f\\\"  - audio_int16_array: {chunk_array.shape}, {len(chunk_bytes)} bytes\\\")\\n                        if len(chunk_bytes) > 0:\\n                            audio_bytes += chunk_bytes\\n                            self.logger.info(f\\\"  - Added {len(chunk_bytes)} bytes to total\\\")\\n                        else:\\n                            self.logger.warning(f\\\"  - audio_int16_array is empty!\\\")\\n                    else:\\n                        self.logger.warning(f\\\"  - AudioChunk {idx} missing expected audio attributes\\\")\\n                        self.logger.warning(f\\\"  - Available attributes: {[attr for attr in dir(chunk) if not attr.startswith('_')]}\\\")\\n                \\n                self.logger.info(f\\\"Extracted audio bytes: {len(audio_bytes)} bytes\\\")\\n                return audio_bytes\\n            else:\\n                self.logger.warning(\\\"No audio chunks received from Piper TTS\\\")\\n                return None\\n        else:\\n            # If it's not a generator, treat as direct bytes\\n            return audio_data\\n    \\n    def _convert_to_wav(self, audio_bytes: bytes) -> bytes:\\n        \\\"\\\"\\\"Convert raw audio bytes to WAV format.\\\"\\\"\\\"\\n        wav_buffer = io.BytesIO()\\n        with wave.open(wav_buffer, 'wb') as wav_file:\\n            wav_file.setnchannels(1)  # Mono\\n            wav_file.setsampwidth(2)  # 16-bit\\n            wav_file.setframerate(TTS_SAMPLE_RATE)  # Piper default sample rate\\n            wav_file.writeframes(audio_bytes)\\n        \\n        return wav_buffer.getvalue()\\n    \\n    async def send_chunked_audio(self, websocket, audio_bytes: bytes, client_address: str, text_response: str) -> bool:\\n        \\\"\\\"\\\"Send audio data in chunks to avoid WebSocket message size limits.\\\"\\\"\\\"\\n        from config import CHUNK_SIZE\\n        \\n        try:\\n            # Send initial metadata\\n            metadata = {\\n                \\\"type\\\": \\\"tts_start\\\",\\n                \\\"text\\\": text_response,\\n                \\\"audio_size\\\": len(audio_bytes),\\n                \\\"chunk_size\\\": CHUNK_SIZE,\\n                \\\"total_chunks\\\": (len(audio_bytes) + CHUNK_SIZE - 1) // CHUNK_SIZE\\n            }\\n            \\n            self.logger.info(f\\\"Sending TTS metadata to {client_address}: {len(audio_bytes)} bytes in {metadata['total_chunks']} chunks\\\")\\n            await safe_send_response(websocket, str(metadata), client_address)\\n            \\n            # Send audio data in chunks\\n            chunk_count = 0\\n            for i in range(0, len(audio_bytes), CHUNK_SIZE):\\n                chunk = audio_bytes[i:i + CHUNK_SIZE]\\n                chunk_count += 1\\n                \\n                self.logger.info(f\\\"Sending audio chunk {chunk_count}/{metadata['total_chunks']} ({len(chunk)} bytes) to {client_address}\\\")\\n                await safe_send_response(websocket, chunk, client_address)\\n            \\n            # Send completion marker\\n            completion = {\\\"type\\\": \\\"tts_end\\\", \\\"chunks_sent\\\": chunk_count}\\n            self.logger.info(f\\\"Sending TTS completion marker to {client_address}\\\")\\n            await safe_send_response(websocket, str(completion), client_address)\\n            \\n            self.logger.info(f\\\"Successfully sent {len(audio_bytes)} bytes of audio in {chunk_count} chunks to {client_address}\\\")\\n            return True\\n            \\n        except Exception as e:\\n            self.logger.error(f\\\"Error sending chunked audio to {client_address}: {e}\\\")\\n            return False\\n\\n\\n# Global audio processor instance\\naudio_processor = AudioProcessor()\\n\"}],\"isError\":false}",
            "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
          },
          "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
        },
        {
          "id": "msg-611778",
          "subType": "code_analysis",
          "type": "text",
          "content": "<p>I will open the vicidial bridge's core files to inspect how it streams audio and handles language/ASR, especially any locale or language parameters that could cause random Kannada/Tamil output.</p>",
          "originalContent": "I will open the vicidial bridge's core files to inspect how it streams audio and handles language/ASR, especially any locale or language parameters that could cause random Kannada/Tamil output.",
          "isTyping": true
        },
        {
          "id": "call_qH7wpgcDyA3k4AMOOc8kT3os",
          "type": "tool_use",
          "subType": "tool_use",
          "isTyping": false,
          "toolUseData": {
            "reasoning": " ",
            "tool": "read_file",
            "serverName": "File System",
            "args": "{\"path\":\"/home/novel/vicidial_bridge (22-10-2025)/bridge.py\"}",
            "identifier": "call_qH7wpgcDyA3k4AMOOc8kT3os",
            "pendingApproval": false,
            "serverType": "BUILTIN",
            "isError": false,
            "message": "{\"content\":[{\"type\":\"text\",\"text\":\"\\\"\\\"\\\"\\nMain bridge module for Vicidial Bridge\\n\\nContains the VicialLocalModelBridge class that orchestrates the entire voice bot system.\\n\\\"\\\"\\\"\\n\\nimport asyncio\\nimport logging\\nimport os\\nimport struct\\nimport time\\nimport numpy as np\\nfrom array import array\\nfrom datetime import datetime\\nfrom typing import Optional, Tuple, Dict, Any\\n\\ntry:\\n    from .config import (\\n        VICIAL_AUDIOSOCKET_HOST, VICIAL_AUDIOSOCKET_PORT, LOCAL_MODEL_WS_URL,\\n        TARGET_EXTENSION, VICIAL_SAMPLE_RATE, LOCAL_MODEL_SAMPLE_RATE,\\n        AUDIO_PROCESS_INTERVAL, MIN_AUDIO_CHUNK_SIZE, MAX_AUDIO_CHUNK_SIZE,\\n        VAD_ENABLED, VAD_ENERGY_THRESHOLD, VAD_SILENCE_DURATION_MS, VAD_MIN_SPEECH_DURATION_MS,\\n        VAD_DEBUG_LOGGING, VAD_HIGH_PASS_CUTOFF, VAD_MIN_CONSECUTIVE_FRAMES, VAD_SPECTRAL_FLATNESS_THRESHOLD,\\n        VAD_BARGE_IN_CONSECUTIVE_FRAMES, VAD_BARGE_IN_ENABLED,\\n        SAVE_COMPLETE_CALLS, SAVE_AI_AUDIO, AI_AUDIO_DIR, validate_environment\\n    )\\n    from .audio_processing import AudioProcessor, AudioSocketProtocol\\n    from .voice_activity_detection import VoiceActivityDetector\\n    from .call_recording import CallRecorder\\n    from .websocket_client import LocalModelWebSocketClient\\n    from .bridge_methods import BridgeMethods\\nexcept ImportError:\\n    from config import (\\n        VICIAL_AUDIOSOCKET_HOST, VICIAL_AUDIOSOCKET_PORT, LOCAL_MODEL_WS_URL,\\n        TARGET_EXTENSION, VICIAL_SAMPLE_RATE, LOCAL_MODEL_SAMPLE_RATE,\\n        AUDIO_PROCESS_INTERVAL, MIN_AUDIO_CHUNK_SIZE, MAX_AUDIO_CHUNK_SIZE,\\n        VAD_ENABLED, VAD_ENERGY_THRESHOLD, VAD_SILENCE_DURATION_MS, VAD_MIN_SPEECH_DURATION_MS,\\n        VAD_DEBUG_LOGGING, VAD_HIGH_PASS_CUTOFF, VAD_MIN_CONSECUTIVE_FRAMES, VAD_SPECTRAL_FLATNESS_THRESHOLD,\\n        VAD_BARGE_IN_CONSECUTIVE_FRAMES, VAD_BARGE_IN_ENABLED,\\n        SAVE_COMPLETE_CALLS, SAVE_AI_AUDIO, AI_AUDIO_DIR, validate_environment\\n    )\\n    from audio_processing import AudioProcessor, AudioSocketProtocol\\n    from voice_activity_detection import VoiceActivityDetector\\n    from call_recording import CallRecorder\\n    from websocket_client import LocalModelWebSocketClient\\n    from bridge_methods import BridgeMethods\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass VicialLocalModelBridge:\\n    \\\"\\\"\\\"\\n    Main bridge class that handles connections from Vicial and routes them to local model server\\n    Manages audio processing, WebSocket communication, and call lifecycle with optimized latency\\n    \\\"\\\"\\\"\\n    \\n    def __init__(self):\\n        self.audio_processor = AudioProcessor()\\n        self.active_calls: Dict[str, Dict[str, Any]] = {}\\n        self.server = None\\n        self.stats = {\\n            'total_calls': 0,\\n            'active_calls': 0,\\n            'successful_calls': 0,\\n            'failed_calls': 0,\\n            'vad_speech_segments': 0,\\n            'vad_silence_discarded': 0,\\n            'start_time': time.time()\\n        }\\n        # Initialize helper methods\\n        self.methods = BridgeMethods(self)\\n        logger.info(\\\"VicialLocalModelBridge initialized\\\")\\n    \\n    async def handle_vicial_connection(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter):\\n        \\\"\\\"\\\"\\n        Handle incoming connection from Vicial AudioSocket with proper protocol framing\\n        This is called when someone dials extension 8888\\n        \\\"\\\"\\\"\\n        client_addr = writer.get_extra_info('peername')\\n        call_id = f\\\"{client_addr[0]}:{client_addr[1]}:{int(time.time())}\\\"\\n        call_uuid = None\\n        \\n        logger.info(f\\\"[{call_id}] New call from Vicial: {client_addr}\\\")\\n        \\n        # Update statistics\\n        self.stats['total_calls'] += 1\\n        self.stats['active_calls'] += 1\\n        \\n        # Create local model client\\n        local_model_client = LocalModelWebSocketClient()\\n        \\n        # Check local model server health before proceeding\\n        if not await local_model_client.health_check():\\n            logger.error(f\\\"[{call_id}] Local model server is not accessible - rejecting call\\\")\\n            await self.methods._send_error_tone(writer)\\n            await self.methods._cleanup_call(call_id, writer)\\n            return\\n        \\n        # Start local model call\\n        if not await local_model_client.start_call():\\n            logger.error(f\\\"[{call_id}] Failed to start local model call - rejecting\\\")\\n            await self.methods._send_error_tone(writer)\\n            await self.methods._cleanup_call(call_id, writer)\\n            return\\n        \\n        # Initialize VAD for this call\\n        vad = None\\n        if VAD_ENABLED:\\n            vad = VoiceActivityDetector(\\n                energy_threshold=VAD_ENERGY_THRESHOLD,\\n                silence_duration_ms=VAD_SILENCE_DURATION_MS,\\n                min_speech_duration_ms=VAD_MIN_SPEECH_DURATION_MS,\\n                sample_rate=VICIAL_SAMPLE_RATE,\\n                high_pass_cutoff=VAD_HIGH_PASS_CUTOFF,\\n                min_consecutive_frames=VAD_MIN_CONSECUTIVE_FRAMES,\\n                spectral_flatness_threshold=VAD_SPECTRAL_FLATNESS_THRESHOLD\\n            )\\n            # Configure barge-in detection for sustained speech\\n            if VAD_BARGE_IN_ENABLED:\\n                # Set minimum sustained speech duration for barge-in (1 second)\\n                vad.min_sustained_speech_duration = 1.0\\n                logger.info(f\\\"[{call_id}] Enhanced VAD enabled with sustained speech barge-in: threshold={VAD_ENERGY_THRESHOLD}, \\\"\\n                           f\\\"silence={VAD_SILENCE_DURATION_MS}ms, min_speech={VAD_MIN_SPEECH_DURATION_MS}ms, \\\"\\n                           f\\\"high_pass={VAD_HIGH_PASS_CUTOFF}Hz, consecutive_frames={VAD_MIN_CONSECUTIVE_FRAMES}, \\\"\\n                           f\\\"sustained_speech_duration={vad.min_sustained_speech_duration}s\\\")\\n            else:\\n                logger.info(f\\\"[{call_id}] Enhanced VAD enabled: threshold={VAD_ENERGY_THRESHOLD}, \\\"\\n                           f\\\"silence={VAD_SILENCE_DURATION_MS}ms, min_speech={VAD_MIN_SPEECH_DURATION_MS}ms, \\\"\\n                           f\\\"high_pass={VAD_HIGH_PASS_CUTOFF}Hz, consecutive_frames={VAD_MIN_CONSECUTIVE_FRAMES}\\\")\\n        else:\\n            logger.info(f\\\"[{call_id}] VAD disabled - processing all audio chunks\\\")\\n\\n        # Initialize call recorder for complete call recording\\n        call_recorder = CallRecorder(call_id) if SAVE_COMPLETE_CALLS else None\\n        \\n        # Store call information with optimized audio buffering and VAD\\n        self.active_calls[call_id] = {\\n            'vicial_reader': reader,\\n            'vicial_writer': writer,\\n            'local_model_client': local_model_client,\\n            'vad': vad,                            # Voice Activity Detector\\n            'call_recorder': call_recorder,        # Complete call recording\\n            'start_time': time.time(),\\n            'audio_frames_sent': 0,\\n            'audio_frames_received': 0,\\n            'ai_responses_received': 0,            # Track AI responses\\n            'tts_requests_sent': 0,                # Track TTS requests\\n            'tts_responses_received': 0,           # Track TTS responses\\n            'last_activity': time.time(),\\n            'audio_buffer': bytearray(),           # Audio buffer for streaming\\n            'processing_audio': False,             # Audio processing flag\\n            'call_uuid': None,                     # Store AudioSocket UUID\\n            'test_tones_sent': 0,                  # Track test tones sent\\n            'last_test_tone_time': 0,              # Track last test tone time\\n            'vad_speech_segments': 0,              # Track VAD speech segments\\n            'vad_silence_discarded': 0,            # Track VAD silence discarded\\n            'processing_tasks': [],                # Track processing tasks for cancellation\\n            'call_ended': False,                   # Flag to indicate call has ended\\n            # BARGE-IN SUPPORT: Turn management and TTS control\\n            'current_turn_id': 0,                  # Current conversation turn\\n            'tts_playback_task': None,             # Currently running TTS playback task\\n            'audio_sending_in_progress': False,    # Flag to prevent race conditions\\n            'audio_streaming_complete': True,      # Flag to track audio completion\\n            'audio_streaming_failed': False,       # Flag to track audio failures\\n            'barge_in_count': 0,                   # Count of barge-in events\\n            'last_barge_in_time': 0                # Timestamp of last barge-in\\n        }\\n        \\n        logger.info(f\\\"[{call_id}] Call setup complete - starting audio processing\\\")\\n        \\n        # DEBUG: Inject test audio to verify AudioSocket is working\\n        await self.methods._inject_test_audio(call_id, writer)\\n        \\n        try:\\n            # Start bidirectional audio processing with task tracking\\n            processing_tasks = [\\n                asyncio.create_task(self._process_vicial_audio(call_id)),\\n                asyncio.create_task(self.methods._process_local_model_responses(call_id)),\\n                asyncio.create_task(self.methods._monitor_call_health(call_id))\\n            ]\\n            \\n            # Store tasks for potential cancellation\\n            self.active_calls[call_id]['processing_tasks'] = processing_tasks\\n            \\n            # Wait for all tasks to complete\\n            await asyncio.gather(*processing_tasks, return_exceptions=True)\\n        except Exception as e:\\n            logger.error(f\\\"[{call_id}] Error during call processing: {e}\\\")\\n        finally:\\n            # CRITICAL: Always cleanup call and save recordings, regardless of how call ended\\n            # This will run even after immediate cleanup\\n            if call_id in self.active_calls:\\n                logger.info(f\\\"[{call_id}] Call processing ended - starting final cleanup and recording save\\\")\\n                await self.methods._cleanup_call(call_id, writer)\\n            else:\\n                logger.warning(f\\\"[{call_id}] Call processing ended - call was already removed from active_calls!\\\")\\n    \\n    async def _process_vicial_audio(self, call_id: str):\\n        \\\"\\\"\\\"Process audio from Vicial by buffering and sending to Local Model\\\"\\\"\\\"\\n        call_info = self.active_calls.get(call_id)\\n        if not call_info:\\n            return\\n        \\n        reader = call_info['vicial_reader']\\n        local_model_client = call_info['local_model_client']\\n        \\n        logger.info(f\\\"[{call_id}] Starting Vicial audio processing\\\")\\n        \\n        # LATENCY OPTIMIZATION: Use optimized audio processing parameters\\n        # Process audio every 50ms for near real-time streaming instead of 200ms\\n        PROCESS_INTERVAL = AUDIO_PROCESS_INTERVAL\\n        MIN_BUFFER_SIZE = MIN_AUDIO_CHUNK_SIZE\\n        MAX_BUFFER_SIZE = MAX_AUDIO_CHUNK_SIZE\\n        \\n        try:\\n            frame_count = 0\\n            last_process_time = time.time()\\n            \\n            # First, read UUID message from AudioSocket\\n            logger.info(f\\\"[{call_id}] Waiting for AudioSocket UUID...\\\")\\n            msg_type, payload = await AudioSocketProtocol.read_frame(reader)\\n            if msg_type == AudioSocketProtocol.TYPE_UUID and len(payload) == 16:\\n                call_uuid = payload.hex()\\n                call_info['call_uuid'] = call_uuid  # Store UUID in call info\\n                logger.info(f\\\"[{call_id}] AudioSocket UUID received: {call_uuid[:8]}...\\\")\\n            else:\\n                logger.warning(f\\\"[{call_id}] Unexpected first message: type={msg_type}, length={len(payload) if payload else 0}\\\")\\n            \\n            while call_id in self.active_calls:\\n                frame_count += 1\\n                \\n                # Read framed audio data from AudioSocket\\n                msg_type, payload = await AudioSocketProtocol.read_frame(reader)\\n                \\n                if msg_type is None:  # Connection closed\\n                    logger.info(f\\\"[{call_id}] AudioSocket connection closed - call ended naturally\\\")\\n                    await self._immediate_call_cleanup(call_id)\\n                    break\\n                elif msg_type == AudioSocketProtocol.TYPE_HANGUP:\\n                    logger.info(f\\\"[{call_id}] AudioSocket hangup received - call ended naturally\\\")\\n                    await self._immediate_call_cleanup(call_id)\\n                    break\\n                elif msg_type == AudioSocketProtocol.TYPE_AUDIO:\\n                    audio_data = payload\\n                elif msg_type == AudioSocketProtocol.TYPE_DTMF:\\n                    logger.debug(f\\\"[{call_id}] DTMF received: {payload}\\\")\\n                    continue  # Skip DTMF for now\\n                else:\\n                    logger.debug(f\\\"[{call_id}] Unknown AudioSocket message type: {msg_type}\\\")\\n                    continue\\n                \\n                if not audio_data:\\n                    continue\\n                \\n                # Update activity timestamp\\n                call_info['last_activity'] = time.time()\\n                call_info['audio_frames_sent'] += 1\\n                \\n                # Process audio from Vicial\\n                logger.info(f\\\"[{call_id}] 📥 Received {len(audio_data)} bytes of audio from Vicial\\\")\\n                \\n                # COMPLETE CALL RECORDING: Add Vicial audio to call recorder\\n                call_recorder = call_info.get('call_recorder')\\n                if call_recorder:\\n                    call_recorder.add_vicial_audio(audio_data)\\n                \\n                # CRITICAL DEBUG: Save raw incoming audio for analysis\\n                await self.audio_processor.save_debug_audio(call_id, audio_data, \\\"vicial_raw_8k\\\", VICIAL_SAMPLE_RATE)\\n                \\n                # Log audio characteristics for debugging\\n                samples = len(audio_data) // 2\\n                duration_ms = (samples / VICIAL_SAMPLE_RATE) * 1000\\n                max_amplitude = max(abs(s) for s in struct.unpack(f'<{samples}h', audio_data)) if samples > 0 else 0\\n                logger.info(f\\\"[{call_id}] 📊 Audio stats: {samples} samples, {duration_ms:.1f}ms, max_amp={max_amplitude}\\\")\\n                \\n                # CRITICAL DEBUG: Analyze first 20 samples of raw audio\\n                audio_array = np.frombuffer(audio_data, dtype=np.int16)\\n                first_20_samples = audio_array[:20].tolist()\\n                logger.info(f\\\"[{call_id}] 🔍 Raw Vicial audio first 20 samples: {first_20_samples}\\\")\\n                \\n                # VOICE ACTIVITY DETECTION: Process audio through VAD\\n                vad = call_info.get('vad')\\n                if vad:\\n                    # REAL-TIME BARGE-IN: Check for speech onset immediately on each chunk\\n                    if VAD_BARGE_IN_ENABLED:\\n                        is_speech_now = vad.is_speech_for_barge_in(audio_data)\\n                    else:\\n                        is_speech_now = vad.is_speech(audio_data)\\n                    \\n                    # If sustained speech is detected and audio is playing, cancel audio immediately (sustained barge-in)\\n                    # This requires 1+ seconds of continuous speech to prevent false triggers from brief noise\\n                    current_time = time.time()\\n                    time_since_last_barge_in = current_time - call_info.get('last_barge_in_time', 0)\\n                    \\n                    # DEBUG: Log barge-in detection status\\n                    tts_task = call_info.get('tts_playback_task')\\n                    if VAD_DEBUG_LOGGING:\\n                        logger.debug(f\\\"[{call_id}] Barge-in check: is_speech_now={is_speech_now}, \\\"\\n                                   f\\\"tts_task_exists={tts_task is not None}, \\\"\\n                                   f\\\"tts_task_done={tts_task.done() if tts_task else 'N/A'}, \\\"\\n                                   f\\\"time_since_last_barge_in={time_since_last_barge_in:.2f}s\\\")\\n                    \\n                    if (is_speech_now and \\n                        tts_task and \\n                        not tts_task.done() and\\n                        time_since_last_barge_in > 2.0):  # 2 second debounce for sustained speech\\n                        \\n                        call_info['barge_in_count'] += 1\\n                        call_info['last_barge_in_time'] = current_time\\n                        logger.info(f\\\"[{call_id}] 🚨 SUSTAINED SPEECH BARGE-IN: User has been speaking for 1+ seconds, cancelling AI playback\\\")\\n                        \\n                        # Cancel current audio playback immediately\\n                        call_info['tts_playback_task'].cancel()\\n                        call_info['tts_playback_task'] = None\\n                        \\n                        # Increment turn ID to invalidate any pending responses\\n                        call_info['current_turn_id'] += 1\\n                        new_turn_id = call_info['current_turn_id']\\n                        \\n                        logger.info(f\\\"[{call_id}] 🔄 Turn ID incremented to {new_turn_id} - old responses will be ignored\\\")\\n                        \\n                        # Clear any pending audio buffers\\n                        call_info['audio_buffer'] = bytearray()\\n                        call_info['audio_sending_in_progress'] = False\\n                        call_info['audio_streaming_complete'] = True\\n                        \\n                        # Reset VAD sustained speech state to prevent immediate re-triggering\\n                        vad.reset_sustained_speech()\\n                        \\n                        # Send a brief silence to flush the audio buffer\\n                        await self._send_silence_flush(call_id, call_info['vicial_writer'])\\n                    \\n                    # Process audio chunk through VAD for complete speech segments\\n                    speech_segment = vad.process_audio_chunk(audio_data)\\n                    \\n                    if speech_segment:\\n                        # VAD detected complete speech segment\\n                        call_info['vad_speech_segments'] += 1\\n                        logger.info(f\\\"[{call_id}] 🎤 VAD detected speech segment #{call_info['vad_speech_segments']}: \\\"\\n                                   f\\\"{len(speech_segment)} bytes\\\")\\n                        \\n                        # Upsample speech segment to 16kHz\\n                        upsampled_audio = self.audio_processor.upsample_8k_to_16k(speech_segment)\\n                        if upsampled_audio:\\n                            # CRITICAL DEBUG: Save upsampled speech segment for analysis\\n                            await self.audio_processor.save_debug_audio(call_id, upsampled_audio, \\\"vad_speech_16k\\\", LOCAL_MODEL_SAMPLE_RATE)\\n                            \\n                            # Send complete speech segment to Local Model\\n                            asyncio.create_task(self.methods._stream_audio_chunk(call_id, upsampled_audio))\\n                        else:\\n                            logger.warning(f\\\"[{call_id}] ⚠️ Upsampling VAD speech segment returned empty audio!\\\")\\n                    else:\\n                        # VAD filtered out silence/noise\\n                        call_info['vad_silence_discarded'] += 1\\n                        if VAD_DEBUG_LOGGING:\\n                            logger.debug(f\\\"[{call_id}] 🔇 VAD filtered silence #{call_info['vad_silence_discarded']} \\\"\\n                                       f\\\"(total discarded: {call_info['vad_silence_discarded']})\\\")\\n                else:\\n                    # VAD disabled - process all audio chunks (original behavior)\\n                    logger.debug(f\\\"[{call_id}] VAD disabled - processing all audio chunks\\\")\\n                    \\n                    # Upsample to 16kHz and add to audio buffer\\n                    upsampled_audio = self.audio_processor.upsample_8k_to_16k(audio_data)\\n                    if upsampled_audio:\\n                        call_info['audio_buffer'].extend(upsampled_audio)\\n                        logger.info(f\\\"[{call_id}] 📥 Added {len(upsampled_audio)} bytes to buffer (total: {len(call_info['audio_buffer'])} bytes)\\\")\\n                        \\n                        # CRITICAL DEBUG: Save upsampled audio for analysis\\n                        await self.audio_processor.save_debug_audio(call_id, upsampled_audio, \\\"upsampled_16k\\\", LOCAL_MODEL_SAMPLE_RATE)\\n                    else:\\n                        logger.warning(f\\\"[{call_id}] ⚠️ Upsampling returned empty audio!\\\")\\n                    \\n                    # LATENCY OPTIMIZATION: Stream audio as soon as minimum buffer is reached\\n                    # Instead of waiting for fixed intervals, send audio immediately when ready\\n                    current_time = time.time()\\n                    buffer_size = len(call_info['audio_buffer'])\\n                    \\n                    # Send audio immediately if we have enough data, or wait for process interval\\n                    should_process = (\\n                        buffer_size >= MIN_BUFFER_SIZE or  # Send immediately when minimum reached\\n                        (current_time - last_process_time >= PROCESS_INTERVAL and buffer_size > 0) or  # Or at interval\\n                        buffer_size >= MAX_BUFFER_SIZE  # Or when buffer is getting too large\\n                    )\\n                    \\n                    if should_process and not call_info['processing_audio']:\\n                        # Mark as processing to prevent concurrent processing\\n                        call_info['processing_audio'] = True\\n                        last_process_time = current_time\\n                        \\n                        # Extract audio chunk to process (don't exceed max buffer size)\\n                        chunk_size = min(buffer_size, MAX_BUFFER_SIZE)\\n                        audio_chunk = bytes(call_info['audio_buffer'][:chunk_size])\\n                        call_info['audio_buffer'] = call_info['audio_buffer'][chunk_size:]\\n                        \\n                        # Calculate actual audio duration for logging\\n                        audio_duration_ms = (len(audio_chunk) / (LOCAL_MODEL_SAMPLE_RATE * 2)) * 1000\\n                        \\n                        logger.debug(f\\\"[{call_id}] 🔄 Streaming {len(audio_chunk)} bytes ({audio_duration_ms:.1f}ms) to Local Model\\\")\\n                        \\n                        # CRITICAL DEBUG: Save audio chunk being sent to Ultravox\\n                        await self.audio_processor.save_debug_audio(call_id, audio_chunk, \\\"sent_to_ultravox_16k\\\", LOCAL_MODEL_SAMPLE_RATE)\\n                        \\n                        # LATENCY OPTIMIZATION: Send audio to Local Model immediately (non-blocking)\\n                        # Use create_task for concurrent processing without blocking the main loop\\n                        asyncio.create_task(self.methods._stream_audio_chunk(call_id, audio_chunk))\\n                    \\n        except asyncio.TimeoutError:\\n            logger.debug(f\\\"[{call_id}] Audio read timeout (normal)\\\")\\n        except Exception as e:\\n            logger.error(f\\\"[{call_id}] Error in audio processing: {e}\\\")\\n        finally:\\n            # VAD CLEANUP: Flush any remaining speech from VAD buffer\\n            if call_id in self.active_calls:\\n                call_info = self.active_calls[call_id]\\n                vad = call_info.get('vad')\\n                if vad:\\n                    remaining_speech = vad.flush_remaining_speech()\\n                    if remaining_speech:\\n                        logger.info(f\\\"[{call_id}] 🎤 VAD flushed remaining speech: {len(remaining_speech)} bytes\\\")\\n                        \\n                        # Upsample and send remaining speech\\n                        upsampled_audio = self.audio_processor.upsample_8k_to_16k(remaining_speech)\\n                        if upsampled_audio:\\n                            asyncio.create_task(self.methods._stream_audio_chunk(call_id, upsampled_audio))\\n            \\n            # Check if call was already cleaned up by immediate cleanup\\n            if call_id in self.active_calls and not self.active_calls[call_id].get('call_ended', False):\\n                logger.info(f\\\"[{call_id}] Audio processing ended - cleanup will be handled by main handler\\\")\\n            else:\\n                logger.info(f\\\"[{call_id}] Audio processing ended - call already cleaned up\\\")\\n    \\n    async def start_server(self):\\n        \\\"\\\"\\\"Start the AudioSocket server for Vicial connections\\\"\\\"\\\"\\n        logger.info(\\\"Starting Vicial-Local Model bridge server...\\\")\\n        \\n        # Validate environment configuration\\n        try:\\n            validate_environment()\\n        except ValueError as e:\\n            logger.error(str(e))\\n            return False\\n        \\n        # Test local model server connectivity\\n        test_client = LocalModelWebSocketClient()\\n        if not await test_client.health_check():\\n            logger.error(\\\"Cannot start bridge - Local model server is not accessible\\\")\\n            logger.error(f\\\"Please check your local model server at {LOCAL_MODEL_HOST}:{LOCAL_MODEL_PORT}\\\")\\n            return False\\n        \\n        try:\\n            self.server = await asyncio.start_server(\\n                self.handle_vicial_connection,\\n                VICIAL_AUDIOSOCKET_HOST,\\n                VICIAL_AUDIOSOCKET_PORT\\n            )\\n            \\n            addr = self.server.sockets[0].getsockname()\\n            logger.info(\\\"=\\\" * 80)\\n            logger.info(\\\"VICIAL-LOCAL MODEL BRIDGE SERVER STARTED\\\")\\n            logger.info(\\\"=\\\" * 80)\\n            logger.info(f\\\"AudioSocket listening on: {addr[0]}:{addr[1]}\\\")\\n            logger.info(f\\\"Target extension: {TARGET_EXTENSION}\\\")\\n            logger.info(f\\\"Local Model Server: {LOCAL_MODEL_WS_URL}\\\")\\n            logger.info(\\\"LATENCY OPTIMIZATION: 50ms audio processing for near real-time streaming\\\")\\n            logger.info(\\\"AUDIO PROCESSING: Direct audio from Ultravox (no TTS conversion needed)\\\")\\n            logger.info(\\\"TTS ENGINE: Piper neural voices for high-quality speech synthesis (fallback)\\\")\\n            logger.info(\\\"Environment: Configuration loaded from .env file\\\")\\n            logger.info(\\\"Ready to handle voice calls!\\\")\\n            logger.info(\\\"=\\\" * 80)\\n            \\n            # Start statistics logging\\n            asyncio.create_task(self._log_statistics())\\n            \\n            # Wait for server to be closed\\n            async with self.server:\\n                await self.server.serve_forever()\\n                \\n        except KeyboardInterrupt:\\n            logger.info(\\\"Server stopped by user\\\")\\n        except Exception as e:\\n            logger.error(f\\\"Server error: {e}\\\")\\n            return False\\n        finally:\\n            await self._shutdown()\\n    \\n    async def _log_statistics(self):\\n        \\\"\\\"\\\"Periodically log server statistics\\\"\\\"\\\"\\n        while True:\\n            try:\\n                await asyncio.sleep(60)  # Log every minute\\n                \\n                uptime = time.time() - self.stats['start_time']\\n                audio_stats = self.audio_processor.get_stats()\\n                \\n                logger.info(\\\"=\\\" * 50)\\n                logger.info(\\\"BRIDGE STATISTICS\\\")\\n                logger.info(\\\"=\\\" * 50)\\n                logger.info(f\\\"Uptime: {uptime/3600:.1f} hours\\\")\\n                logger.info(f\\\"Total calls: {self.stats['total_calls']}\\\")\\n                logger.info(f\\\"Active calls: {self.stats['active_calls']}\\\")\\n                logger.info(f\\\"Successful calls: {self.stats['successful_calls']}\\\")\\n                logger.info(f\\\"Failed calls: {self.stats['failed_calls']}\\\")\\n                logger.info(f\\\"Audio frames processed: {audio_stats['frames_upsampled']} up, {audio_stats['frames_downsampled']} down\\\")\\n                logger.info(f\\\"Audio data processed: {audio_stats['total_input_bytes']/1024/1024:.1f}MB in, {audio_stats['total_output_bytes']/1024/1024:.1f}MB out\\\")\\n                logger.info(f\\\"VAD processing: {self.stats['vad_speech_segments']} speech segments, {self.stats['vad_silence_discarded']} silence chunks filtered\\\")\\n                if audio_stats['errors'] > 0:\\n                    logger.warning(f\\\"Audio processing errors: {audio_stats['errors']}\\\")\\n                logger.info(\\\"=\\\" * 50)\\n                \\n            except Exception as e:\\n                logger.error(f\\\"Error logging statistics: {e}\\\")\\n    \\n    async def _shutdown(self):\\n        \\\"\\\"\\\"Clean shutdown of the bridge\\\"\\\"\\\"\\n        logger.info(\\\"Shutting down Vicial-Local Model bridge...\\\")\\n        \\n        # Close all active calls\\n        for call_id in list(self.active_calls.keys()):\\n            logger.info(f\\\"Closing active call: {call_id}\\\")\\n            await self.methods._cleanup_call(call_id, self.active_calls[call_id]['vicial_writer'])\\n        \\n        # Close server\\n        if self.server:\\n            self.server.close()\\n            await self.server.wait_closed()\\n        \\n        logger.info(\\\"Bridge shutdown complete\\\")\\n    \\n    async def _immediate_call_cleanup(self, call_id: str):\\n        \\\"\\\"\\\"Perform immediate cleanup when AudioSocket disconnects - but preserve call for recording save\\\"\\\"\\\"\\n        call_info = self.active_calls.get(call_id)\\n        if not call_info or call_info.get('call_ended', False):\\n            logger.debug(f\\\"[{call_id}] Call already cleaned up or ended, skipping immediate cleanup\\\")\\n            return\\n        \\n        logger.info(f\\\"[{call_id}] 🚨 IMMEDIATE CLEANUP: AudioSocket disconnected, marking call as ended\\\")\\n        \\n        # Mark call as ended to prevent further processing\\n        call_info['call_ended'] = True\\n        \\n        # Cancel all processing tasks for this call\\n        processing_tasks = call_info.get('processing_tasks', [])\\n        for task in processing_tasks:\\n            if not task.done() and task != asyncio.current_task():\\n                logger.debug(f\\\"[{call_id}] Cancelling task: {task.get_name()}\\\")\\n                task.cancel()\\n        \\n        # Close WebSocket connection immediately to stop further processing\\n        local_model_client = call_info.get('local_model_client')\\n        if local_model_client:\\n            try:\\n                await local_model_client.close_call()\\n                logger.debug(f\\\"[{call_id}] Closed WebSocket connection\\\")\\n            except Exception as e:\\n                logger.error(f\\\"[{call_id}] Error closing WebSocket: {e}\\\")\\n        \\n        # Close the AudioSocket writer to prevent broken pipe errors\\n        writer = call_info.get('vicial_writer')\\n        if writer and not writer.is_closing():\\n            try:\\n                writer.close()\\n                await writer.wait_closed()\\n                logger.debug(f\\\"[{call_id}] Closed AudioSocket writer\\\")\\n            except Exception as e:\\n                logger.debug(f\\\"[{call_id}] Error closing writer (expected): {e}\\\")\\n        \\n        # DO NOT call _cleanup_call() here - let the main handler do it\\n        # DO NOT remove from active_calls - let the main handler do it\\n        logger.info(f\\\"[{call_id}] ✅ Immediate cleanup completed - call marked as ended, main cleanup will save recording\\\")\\n    \\n    async def _send_silence_flush(self, call_id: str, writer):\\n        \\\"\\\"\\\"Send a brief silence to flush the audio buffer after barge-in\\\"\\\"\\\"\\n        try:\\n            # Send 100ms of silence to flush any remaining audio\\n            silence_duration_ms = 100\\n            silence_samples = int(VICIAL_SAMPLE_RATE * silence_duration_ms / 1000)\\n            silence_bytes = b'\\\\x00' * (silence_samples * 2)  # 16-bit samples\\n            \\n            logger.debug(f\\\"[{call_id}] 🔇 Sending {silence_duration_ms}ms silence flush...\\\")\\n            success = await AudioSocketProtocol.write_audio_frame(writer, silence_bytes)\\n            \\n            if success:\\n                logger.debug(f\\\"[{call_id}] ✅ Silence flush sent successfully\\\")\\n            else:\\n                logger.warning(f\\\"[{call_id}] ⚠️ Failed to send silence flush\\\")\\n                \\n        except Exception as e:\\n            logger.error(f\\\"[{call_id}] Error sending silence flush: {e}\\\")\\n\"}],\"isError\":false}",
            "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
          },
          "sessionId": "20251025-a9cd9c8c-1565-4123-ad6b-3b5db8bab710"
        },
        {
          "id": "msg-655223",
          "subType": "code_analysis",
          "type": "text",
          "content": "<p>Plan</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Inspect server and vicidial bridge code paths that handle streaming ASR/LLM, TTS, and interruption/barge-in.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Identify where language may drift (Kannada/Tamil) when you interrupt mid-utterance.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Propose concrete fixes in config and code to clamp language, avoid cross-talk during barge-in, and sanitize LLM/TTS outputs.</li>\n</ul>\n<p>Findings from code review</p>\n<p>Key server components</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">\n<p>server/websocket_handler.py</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Manages audio messages, generates LLM response with model_manager.process_audio(), and handles TTS with cancellable generation when VAD interruption is enabled.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Implements VAD monitoring during TTS playback and triggers interruption via _trigger_interruption() that cancels TTS and re-enters the audio handling path with buffered audio.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">\n<p>server/models.py (ModelManager)</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Ultravox LLM pipeline is used for speech-in + turns; Whisper-small is used for transcription.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Critical behavior:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Adds a single &lt;|audio|&gt; marker into the last user turn (cleaned to exactly one).</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Ensures first system prompt is: You are Alexa, HR recruiter… English prompt, short, conversational. There is no explicit language constraint instruction to always reply in English nor a format restriction on non-English text.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Inference call: do_sample=True, temperature=0.7, max_new_tokens=2000. This can cause creative drift and code-switching.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Whisper transcription:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Uses openai/whisper-small with default processor/model settings; by default Whisper can auto-detect language unless forced. No language or task set. Auto-lang detection can switch languages under noise/interruptions.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">No explicit TTS language setting beyond using PiperVoice.load(model). If a multilingual Piper voice is used or the voice’s config supports Indian locales, TTS will faithfully synthesize whatever text the LLM outputs.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">\n<p>server/audio_processor.py</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">TTS via Piper, with cancellable generation for interruptions.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">No language guard; it will synthesize whatever text is provided (including occasional Kannada/Tamil strings).</li>\n</ul>\n</li>\n</ul>\n<p>Vicidial bridge components (22-10-2025)</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">vicidial_bridge (22-10-2025)/bridge.py\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Handles 8 kHz incoming audio via AudioSocket, VAD, barge-in logic for “sustained speech” (1+ seconds), upsampling to 16 kHz, and streaming to local model.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">On barge-in: cancels current playback, increments turn_id, flushes buffers, and immediately streams newly detected speech.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">This reduces latency but increases chances of overlapping “AI was speaking + user interrupts” flows that may produce partial segments sent to ASR/LLM.</li>\n</ul>\n</li>\n</ul>\n<p>Root causes of random Kannada/Tamil emissions on interrupt</p>\n<ol style=\"list-style-type: decimal; counter-reset: list-counter;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">\n<p>Whisper language auto-detection flips under noise/short utterances:</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">During barge-in, short/noisy fragments get sent to Whisper. Without a forced language, Whisper may detect a South Indian language (Kannada/Tamil) spuriously and decode into it, polluting the transcript that is fed to the LLM.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">\n<p>LLM language drift under sampling:</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Ultravox is called with do_sample=True, temperature=0.7, no explicit “use English only” policy. Under partially valid transcript or noise, the LLM can emit non-English scripts. Once it drifts, the next turns may keep that style.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">\n<p>Mixed state during interruption:</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">websocket_handler._monitor_for_interruption() reads websocket.recv() concurrently with the normal message loop. If the client keeps sending both “normal audio” and VAD-monitoring audio frames to the same WS without a clear sub-protocol, you can race: partial frames may be processed as if they were valid turns, and the combined buffer in _trigger_interruption() may contain clipped/garbled audio, making ASR more error-prone and triggering language mis-detection.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">\n<p>TTS reads whatever text is returned:</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Piper will synthesize text as-is. If the LLM responds in Kannada/Tamil (because input or context drifted), you’ll hear that, especially on barge-in when the conversation context is mid-stream and not clamped.</li>\n</ul>\n</li>\n</ol>\n<p>Targeted fixes</p>\n<p>A) Force ASR language to English</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Whisper supports setting forced decoder language. With transformers WhisperProcessor/WhisperForConditionalGeneration you can:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Use processor.tokenizer.set_language(\"en\")</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Or pass language/forced_decoder_ids via generate.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">In _transcribe_audio_chunk(), set forced decoder IDs for English and disable task translation missteps.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Additionally, do short-utterance gating: if chunk &lt; N ms and VAD probability isn’t strong, skip or ask for repeat. You already have unified audio validation; combine with VAD presence check server-side to reduce false language detection from partial speech.</li>\n</ul>\n<p>Concrete in server/models.py:</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">After creating whisper_processor and whisper_model, store forced_decoder_ids for English:\nfrom transformers import WhisperProcessor\nforced_decoder_ids = self.whisper_processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">In _transcribe_audio_chunk(), pass forced_decoder_ids=forced_decoder_ids to generate, e.g.:\npredicted_ids = self.whisper_model.generate(\ninput_features,\nmax_length=448,\nforced_decoder_ids=self.whisper_processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n)</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Optional: set self.whisper_processor.tokenizer.set_language(\"en\").</li>\n</ul>\n<p>B) Clamp LLM to English and phone-call register</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Strengthen the Ultravox system prompt to explicitly require English-only output and forbid non-English scripts:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Add: Always respond in English. Never use other languages or scripts. If input seems non-English or unclear, ask for clarification in English.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Reduce randomness during noisy/interrupt transitions:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Consider do_sample=False, or lower temperature to 0.2–0.3 when input audio stats are borderline or when interruption was just handled. You can adapt temperature based on validation stats.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Sanitize outputs before TTS:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Add a simple guard function to check for presence of non-ASCII letters or significant proportion of non-Latin characters and, if found, re-ask the LLM in a correction pass (“Convert to concise English reply to the user’s last utterance; do not translate non-speech noise.”) or replace with a safe fallback like “Sorry, I didn’t catch that—could you please repeat?”.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">You already sanitize &lt;|audio|&gt; placeholders; extend with a language guard.</li>\n</ul>\n</li>\n</ul>\n<p>C) Improve interruption path buffering to avoid garbage ASR input</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">websocket_handler._monitor_for_interruption() concurrently consumes websocket frames for VAD. This competes with handle_connection’s primary async for message loop. Two concurrent consumers of the same WebSocket is brittle.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Safer design:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Let the main message loop receive everything. Run VAD on mirrored copies of incoming audio frames inside the main loop, not via websocket.recv() in the VAD monitor. The VAD monitor can read from a concurrent queue fed by main loop rather than recv() on the same socket.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">When TTS starts, set ai_speaking=True; main loop buffers incoming audio chunks to a queue while also feeding them to VAD; when VAD decides to trigger, cancel TTS, then combine only speech-validated buffered audio (not all raw bytes), ensuring segment quality.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">At minimum, if you keep current design:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Validate buffered audio in _trigger_interruption() before passing to _handle_audio_message: if too short or fails unified validation/VAD speech present, drop it and ask for repeat. That avoids sending garbage to Whisper.</li>\n</ul>\n</li>\n</ul>\n<p>D) VAD-driven gating before calling ASR/LLM on barge-in buffers</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">In _trigger_interruption(), call a VAD check on combined_audio; only proceed if speech_prob high and length &gt; threshold; else ask for repeat.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">In vicidial bridge, VAD already ensures sustained speech for barge-in; apply similar gating on server side for WebSocket-based clients.</li>\n</ul>\n<p>E) Lower the chance of multilingual outputs with decoding constraints</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">If Ultravox supports tokenization constraints or stop-sets, consider using an English-only vocabulary or penalty on non-Latin Unicode ranges. If not available, stick to prompt + temperature and output sanitization fallback.</li>\n</ul>\n<p>F) Consistency in turns and preventing residual placeholders</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">You already added rigorous cleanup around &lt;|audio|&gt; markers. Keep it, it helps prevent malformed prompts that could confuse the LLM.</li>\n</ul>\n<p>G) TTS safety fallback</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Before calling Piper, run validate_text_for_tts to filter non-textual markers; extend it to reject predominantly non-Latin content. If rejected, synthesize an English fallback prompt instead of the multilingual text.</li>\n</ul>\n<p>Actionable code changes to implement</p>\n<ol style=\"list-style-type: decimal; counter-reset: list-counter;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Force Whisper language to English in server/models.py</li>\n</ol>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">After loading the Whisper processor/model in _load_whisper(), keep a cached:\nself.whisper_forced_decoder_ids = self.whisper_processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">In _transcribe_audio_chunk(), call:\npredicted_ids = self.whisper_model.generate(\ninput_features,\nmax_length=448,\nforced_decoder_ids=self.whisper_forced_decoder_ids\n)</li>\n</ul>\n<ol style=\"list-style-type: decimal; counter-reset: list-counter;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Strengthen system prompt and clamp sampling</li>\n</ol>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">In process_audio() where system prompt is inserted, change the content to include:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Always and strictly respond in English only. Never produce non-English text, non-Latin scripts, or transliterations. If user speech is unclear or noisy, ask for a short clarification in English.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Optionally adjust decoding parameters:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">For normal: do_sample=True, temperature=0.5</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">For interrupted/low-confidence: do_sample=False or temperature=0.2. You can pass a parameter from websocket_handler to process_audio when an interruption just occurred to lower temperature adaptively.</li>\n</ul>\n</li>\n</ul>\n<ol style=\"list-style-type: decimal; counter-reset: list-counter;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Add output sanitization before TTS</li>\n</ol>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">In websocket_handler after response_text is obtained, before audio_processor.generate_tts_audio*_:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">If contains a high proportion of non-Latin characters or fails a language guard, replace with “Sorry, I didn’t catch that. Could you please repeat?” and optionally log the original for debugging.</li>\n</ul>\n</li>\n</ul>\n<ol style=\"list-style-type: decimal; counter-reset: list-counter;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">VAD/validation gate on interruption buffer</li>\n</ol>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">In _trigger_interruption(), after combined_audio = b''.join(connection[\"audio_buffer\"]):\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">If not self._should_process_audio(combined_audio) or not self._should_process_audio_with_vad(combined_audio): drop it and respond with a clarification instead of routing to _handle_audio_message. This will prevent garbage fragments reaching Whisper.</li>\n</ul>\n</li>\n</ul>\n<ol style=\"list-style-type: decimal; counter-reset: list-counter;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Avoid concurrent websocket.recv() in both main loop and VAD monitor</li>\n</ol>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Replace _monitor_for_interruption()’s websocket.recv() loop with reading from a thread-safe asyncio.Queue that the main loop feeds with audio frames while ai_speaking=True.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">If restructuring now is too invasive, add stronger gating and shorten the timeout plus minimum chunk accumulation inside _monitor_for_interruption() before triggering.</li>\n</ul>\n<ol style=\"list-style-type: decimal; counter-reset: list-counter;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Vicidial bridge corroboration</li>\n</ol>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">The bridge already requires 1+ seconds sustained speech for barge-in and flushes audio. That is good. Ensure the local model WS endpoint is using the server logic above so that forced English is consistent across both flows. If the bridge has its own ASR path (it appears it streams audio to local model server), the forced-English change will apply in server/models.py.</li>\n</ul>\n<p>Operational recommendations</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Retrain or select the Piper voice strictly paired for English (en_US). Ensure the Piper model files correspond to an English voice. Mixed or Indian locale voices can pronounce Kannada/Tamil text plausibly; they won’t prevent drift without the ASR/LLM clamps.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Monitor and log language drift:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Add a lightweight detector to count Unicode ranges in response before TTS. Log any response flagged as non-English along with the prior user transcription and interruption state. This will quickly show if the issue disappears after forced-decoder IDs.</li>\n</ul>\n</li>\n</ul>\n<p>Why this fixes the issue</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Forced English decoding in Whisper eliminates most mis-detections into Kannada/Tamil under short/noisy clips during barge-in.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Stronger system prompt and lower temperature under interruption reduces LLM creative drift into other scripts even if the transcript is partially garbled.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Sanitization and fallback ensure that, even if a non-English string slips through, TTS won’t propagate it to the caller.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">VAD/validation gating on the interruption buffer prevents tiny garbage segments being fed into ASR/LLM, a frequent cause of wrong-language hallucinations.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Preventing concurrent consumption of the same WebSocket stream reduces partial-frame races that yield broken audio to ASR.</li>\n</ul>\n<p>Execution outline if you want me to implement:</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Update server/models.py to:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Configure forced_decoder_ids for English and pass to Whisper generate.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Upgrade system prompt with “English only” policy and ask-for-clarification rule.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Optionally expose a process_audio(..., conservative=False) flag to reduce temperature after interruptions.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Update server/websocket_handler.py to:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Add language guard on response_text before TTS.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">In _trigger_interruption(), validate combined_audio with VAD and unified validation before forwarding; add safe fallback message if rejected.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Optionally rework interruption monitor to consume from a queue instead of websocket.recv().</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Verify Piper voice is an English voice; if not, switch the model in config to an English Piper voice.</li>\n</ul>\n<p>Run-time validation steps</p>\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Place test calls, interrupt mid-sentence, and observe:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">No non-English output synthesized.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">If input is too short/noisy, bot asks for repetition in English.</li>\n</ul>\n</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Review logs for:\n<ul style=\"list-style-type: disc;\">\n<li style=\"display: list-item; counter-increment: list-counter;\">Whisper forced_decoder_ids engaged.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">LLM parameters and system prompt in prompt_logger.</li>\n<li style=\"display: list-item; counter-increment: list-counter;\">Sanitization fallbacks triggered count (should be near zero after fixes).</li>\n</ul>\n</li>\n</ul>\n<p>This set of changes will stabilize the live calling system under interruptions and eliminate random Kannada/Tamil outputs.</p>",
          "originalContent": "Plan\n- Inspect server and vicidial bridge code paths that handle streaming ASR/LLM, TTS, and interruption/barge-in.\n- Identify where language may drift (Kannada/Tamil) when you interrupt mid-utterance.\n- Propose concrete fixes in config and code to clamp language, avoid cross-talk during barge-in, and sanitize LLM/TTS outputs.\n\nFindings from code review\n\nKey server components\n- server/websocket_handler.py\n  - Manages audio messages, generates LLM response with model_manager.process_audio(), and handles TTS with cancellable generation when VAD interruption is enabled.\n  - Implements VAD monitoring during TTS playback and triggers interruption via _trigger_interruption() that cancels TTS and re-enters the audio handling path with buffered audio.\n\n- server/models.py (ModelManager)\n  - Ultravox LLM pipeline is used for speech-in + turns; Whisper-small is used for transcription.\n  - Critical behavior:\n    - Adds a single <|audio|> marker into the last user turn (cleaned to exactly one).\n    - Ensures first system prompt is: You are Alexa, HR recruiter… English prompt, short, conversational. There is no explicit language constraint instruction to always reply in English nor a format restriction on non-English text.\n    - Inference call: do_sample=True, temperature=0.7, max_new_tokens=2000. This can cause creative drift and code-switching.\n  - Whisper transcription:\n    - Uses openai/whisper-small with default processor/model settings; by default Whisper can auto-detect language unless forced. No language or task set. Auto-lang detection can switch languages under noise/interruptions.\n  - No explicit TTS language setting beyond using PiperVoice.load(model). If a multilingual Piper voice is used or the voice’s config supports Indian locales, TTS will faithfully synthesize whatever text the LLM outputs.\n\n- server/audio_processor.py\n  - TTS via Piper, with cancellable generation for interruptions.\n  - No language guard; it will synthesize whatever text is provided (including occasional Kannada/Tamil strings).\n\nVicidial bridge components (22-10-2025)\n- vicidial_bridge (22-10-2025)/bridge.py\n  - Handles 8 kHz incoming audio via AudioSocket, VAD, barge-in logic for “sustained speech” (1+ seconds), upsampling to 16 kHz, and streaming to local model.\n  - On barge-in: cancels current playback, increments turn_id, flushes buffers, and immediately streams newly detected speech.\n  - This reduces latency but increases chances of overlapping “AI was speaking + user interrupts” flows that may produce partial segments sent to ASR/LLM.\n\nRoot causes of random Kannada/Tamil emissions on interrupt\n1) Whisper language auto-detection flips under noise/short utterances:\n   - During barge-in, short/noisy fragments get sent to Whisper. Without a forced language, Whisper may detect a South Indian language (Kannada/Tamil) spuriously and decode into it, polluting the transcript that is fed to the LLM.\n\n2) LLM language drift under sampling:\n   - Ultravox is called with do_sample=True, temperature=0.7, no explicit “use English only” policy. Under partially valid transcript or noise, the LLM can emit non-English scripts. Once it drifts, the next turns may keep that style.\n\n3) Mixed state during interruption:\n   - websocket_handler._monitor_for_interruption() reads websocket.recv() concurrently with the normal message loop. If the client keeps sending both “normal audio” and VAD-monitoring audio frames to the same WS without a clear sub-protocol, you can race: partial frames may be processed as if they were valid turns, and the combined buffer in _trigger_interruption() may contain clipped/garbled audio, making ASR more error-prone and triggering language mis-detection.\n\n4) TTS reads whatever text is returned:\n   - Piper will synthesize text as-is. If the LLM responds in Kannada/Tamil (because input or context drifted), you’ll hear that, especially on barge-in when the conversation context is mid-stream and not clamped.\n\nTargeted fixes\n\nA) Force ASR language to English\n- Whisper supports setting forced decoder language. With transformers WhisperProcessor/WhisperForConditionalGeneration you can:\n  - Use processor.tokenizer.set_language(\"en\")\n  - Or pass language/forced_decoder_ids via generate.\n- In _transcribe_audio_chunk(), set forced decoder IDs for English and disable task translation missteps.\n- Additionally, do short-utterance gating: if chunk < N ms and VAD probability isn’t strong, skip or ask for repeat. You already have unified audio validation; combine with VAD presence check server-side to reduce false language detection from partial speech.\n\nConcrete in server/models.py:\n- After creating whisper_processor and whisper_model, store forced_decoder_ids for English:\n  from transformers import WhisperProcessor\n  forced_decoder_ids = self.whisper_processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n- In _transcribe_audio_chunk(), pass forced_decoder_ids=forced_decoder_ids to generate, e.g.:\n  predicted_ids = self.whisper_model.generate(\n      input_features,\n      max_length=448,\n      forced_decoder_ids=self.whisper_processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n  )\n- Optional: set self.whisper_processor.tokenizer.set_language(\"en\").\n\nB) Clamp LLM to English and phone-call register\n- Strengthen the Ultravox system prompt to explicitly require English-only output and forbid non-English scripts:\n  - Add: Always respond in English. Never use other languages or scripts. If input seems non-English or unclear, ask for clarification in English.\n- Reduce randomness during noisy/interrupt transitions:\n  - Consider do_sample=False, or lower temperature to 0.2–0.3 when input audio stats are borderline or when interruption was just handled. You can adapt temperature based on validation stats.\n- Sanitize outputs before TTS:\n  - Add a simple guard function to check for presence of non-ASCII letters or significant proportion of non-Latin characters and, if found, re-ask the LLM in a correction pass (“Convert to concise English reply to the user’s last utterance; do not translate non-speech noise.”) or replace with a safe fallback like “Sorry, I didn’t catch that—could you please repeat?”.\n  - You already sanitize <|audio|> placeholders; extend with a language guard.\n\nC) Improve interruption path buffering to avoid garbage ASR input\n- websocket_handler._monitor_for_interruption() concurrently consumes websocket frames for VAD. This competes with handle_connection’s primary async for message loop. Two concurrent consumers of the same WebSocket is brittle.\n- Safer design:\n  - Let the main message loop receive everything. Run VAD on mirrored copies of incoming audio frames inside the main loop, not via websocket.recv() in the VAD monitor. The VAD monitor can read from a concurrent queue fed by main loop rather than recv() on the same socket.\n  - When TTS starts, set ai_speaking=True; main loop buffers incoming audio chunks to a queue while also feeding them to VAD; when VAD decides to trigger, cancel TTS, then combine only speech-validated buffered audio (not all raw bytes), ensuring segment quality.\n- At minimum, if you keep current design:\n  - Validate buffered audio in _trigger_interruption() before passing to _handle_audio_message: if too short or fails unified validation/VAD speech present, drop it and ask for repeat. That avoids sending garbage to Whisper.\n\nD) VAD-driven gating before calling ASR/LLM on barge-in buffers\n- In _trigger_interruption(), call a VAD check on combined_audio; only proceed if speech_prob high and length > threshold; else ask for repeat.\n- In vicidial bridge, VAD already ensures sustained speech for barge-in; apply similar gating on server side for WebSocket-based clients.\n\nE) Lower the chance of multilingual outputs with decoding constraints\n- If Ultravox supports tokenization constraints or stop-sets, consider using an English-only vocabulary or penalty on non-Latin Unicode ranges. If not available, stick to prompt + temperature and output sanitization fallback.\n\nF) Consistency in turns and preventing residual placeholders\n- You already added rigorous cleanup around <|audio|> markers. Keep it, it helps prevent malformed prompts that could confuse the LLM.\n\nG) TTS safety fallback\n- Before calling Piper, run validate_text_for_tts to filter non-textual markers; extend it to reject predominantly non-Latin content. If rejected, synthesize an English fallback prompt instead of the multilingual text.\n\nActionable code changes to implement\n\n1) Force Whisper language to English in server/models.py\n- After loading the Whisper processor/model in _load_whisper(), keep a cached:\n  self.whisper_forced_decoder_ids = self.whisper_processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n- In _transcribe_audio_chunk(), call:\n  predicted_ids = self.whisper_model.generate(\n      input_features,\n      max_length=448,\n      forced_decoder_ids=self.whisper_forced_decoder_ids\n  )\n\n2) Strengthen system prompt and clamp sampling\n- In process_audio() where system prompt is inserted, change the content to include:\n  - Always and strictly respond in English only. Never produce non-English text, non-Latin scripts, or transliterations. If user speech is unclear or noisy, ask for a short clarification in English.\n- Optionally adjust decoding parameters:\n  - For normal: do_sample=True, temperature=0.5\n  - For interrupted/low-confidence: do_sample=False or temperature=0.2. You can pass a parameter from websocket_handler to process_audio when an interruption just occurred to lower temperature adaptively.\n\n3) Add output sanitization before TTS\n- In websocket_handler after response_text is obtained, before audio_processor.generate_tts_audio*_:\n  - If contains a high proportion of non-Latin characters or fails a language guard, replace with “Sorry, I didn’t catch that. Could you please repeat?” and optionally log the original for debugging.\n\n4) VAD/validation gate on interruption buffer\n- In _trigger_interruption(), after combined_audio = b''.join(connection[\"audio_buffer\"]):\n  - If not self._should_process_audio(combined_audio) or not self._should_process_audio_with_vad(combined_audio): drop it and respond with a clarification instead of routing to _handle_audio_message. This will prevent garbage fragments reaching Whisper.\n\n5) Avoid concurrent websocket.recv() in both main loop and VAD monitor\n- Replace _monitor_for_interruption()’s websocket.recv() loop with reading from a thread-safe asyncio.Queue that the main loop feeds with audio frames while ai_speaking=True.\n- If restructuring now is too invasive, add stronger gating and shorten the timeout plus minimum chunk accumulation inside _monitor_for_interruption() before triggering.\n\n6) Vicidial bridge corroboration\n- The bridge already requires 1+ seconds sustained speech for barge-in and flushes audio. That is good. Ensure the local model WS endpoint is using the server logic above so that forced English is consistent across both flows. If the bridge has its own ASR path (it appears it streams audio to local model server), the forced-English change will apply in server/models.py.\n\nOperational recommendations\n- Retrain or select the Piper voice strictly paired for English (en_US). Ensure the Piper model files correspond to an English voice. Mixed or Indian locale voices can pronounce Kannada/Tamil text plausibly; they won’t prevent drift without the ASR/LLM clamps.\n- Monitor and log language drift:\n  - Add a lightweight detector to count Unicode ranges in response before TTS. Log any response flagged as non-English along with the prior user transcription and interruption state. This will quickly show if the issue disappears after forced-decoder IDs.\n\nWhy this fixes the issue\n- Forced English decoding in Whisper eliminates most mis-detections into Kannada/Tamil under short/noisy clips during barge-in.\n- Stronger system prompt and lower temperature under interruption reduces LLM creative drift into other scripts even if the transcript is partially garbled.\n- Sanitization and fallback ensure that, even if a non-English string slips through, TTS won’t propagate it to the caller.\n- VAD/validation gating on the interruption buffer prevents tiny garbage segments being fed into ASR/LLM, a frequent cause of wrong-language hallucinations.\n- Preventing concurrent consumption of the same WebSocket stream reduces partial-frame races that yield broken audio to ASR.\n\nExecution outline if you want me to implement:\n- Update server/models.py to:\n  - Configure forced_decoder_ids for English and pass to Whisper generate.\n  - Upgrade system prompt with “English only” policy and ask-for-clarification rule.\n  - Optionally expose a process_audio(..., conservative=False) flag to reduce temperature after interruptions.\n- Update server/websocket_handler.py to:\n  - Add language guard on response_text before TTS.\n  - In _trigger_interruption(), validate combined_audio with VAD and unified validation before forwarding; add safe fallback message if rejected.\n  - Optionally rework interruption monitor to consume from a queue instead of websocket.recv().\n- Verify Piper voice is an English voice; if not, switch the model in config to an English Piper voice.\n\nRun-time validation steps\n- Place test calls, interrupt mid-sentence, and observe:\n  - No non-English output synthesized.\n  - If input is too short/noisy, bot asks for repetition in English.\n- Review logs for:\n  - Whisper forced_decoder_ids engaged.\n  - LLM parameters and system prompt in prompt_logger.\n  - Sanitization fallbacks triggered count (should be near zero after fixes).\n\nThis set of changes will stabilize the live calling system under interruptions and eliminate random Kannada/Tamil outputs.",
          "isTyping": true
        }
      ]
    }
  ]
}